{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "import heapq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "# import matplotlib.animation as animation\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGSN_Env(gym.Env):\n",
    "    def __init__(self, grid_size=10, max_static_obstacles=30, max_dynamic_obstacles=20, max_steps=1000):\n",
    "        super(DGSN_Env, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.max_static_obstacles = max_static_obstacles\n",
    "        self.max_dynamic_obstacles = max_dynamic_obstacles\n",
    "        # self.num_static_obstacles = np.random.randint(1,10)\n",
    "        # self.num_dynamic_obstacles = np.random.randint(1,21)\n",
    "        # self.agent_pos = np.array(np.random.uniform(0, self.grid_size, size=2).round(1))\n",
    "        # self.goal_pos = np.array(np.random.uniform(0, self.grid_size, size=2).round(1))\n",
    "        self.num_static_obstacles = 30\n",
    "        self.num_dynamic_obstacles = 20\n",
    "\n",
    "        self.agent_pos = np.array(np.random.randint(0, self.grid_size, size=2))\n",
    "        self.goal_pos = np.array(np.random.randint(0, self.grid_size, size=2))\n",
    "        \n",
    "        self.dynamic_obstacles = self._init_dynamic_obstacles()\n",
    "        self.static_obstacles = self._init_static_obstacles()\n",
    "        \n",
    "        self.close_call, self.discomfort, self.current_step, self.ep_no = 0, 0, 1, 0\n",
    "        self.dist_factor = 2\n",
    "        self.max_steps = max_steps\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        self.action_space = spaces.Discrete(9)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'agent': spaces.Box(low=0, high=grid_size, shape=(2,), dtype=np.float32),\n",
    "            'dyn_obs': spaces.Box(low=0, high=grid_size, shape=(self.max_dynamic_obstacles, 2), dtype=np.float32),\n",
    "            'sta_obs': spaces.Box(low=0, high=grid_size, shape=(self.max_static_obstacles, 2), dtype=np.float32)\n",
    "        })\n",
    "        \n",
    "        wandb.login()\n",
    "        wandb.init(project='DGSN_runs')\n",
    "        \n",
    "        self.safe = True\n",
    "        \n",
    "        # Evaluation Metrics\n",
    "        self.Avg_Success_Rate = 0\n",
    "        self.Avg_Collision_Rate, self.ep_collision_rate, self.collision_count = 0, 0, 0\n",
    "        self.Avg_Min_Time_To_Collision, self.min_time_to_collision = 0, 0\n",
    "        self.Avg_Wall_Collision_Rate, self.wall_collision_count = 0, 0\n",
    "        self.Avg_Obstacle_Collision_Rate, self.obstacle_collision_count = 0, 0\n",
    "        self.Avg_Human_Collision_Rate, self.human_collision_count = 0, 0\n",
    "\n",
    "        self.Avg_Timeout = 0\n",
    "        self.Avg_Path_Length = 0\n",
    "        self.Avg_Stalled_Time, self.stalled_time = 0, 0\n",
    "\n",
    "        self.Avg_Discomfort, self.ep_discomfort = 0, 0\n",
    "        self.Avg_Human_Distance, self.ep_human_distance, self.human_distance = 0, 0, 0\n",
    "        self.Avg_Closest_Human_Distance, self.closest_human_distance = 0, 0\n",
    "        self.Min_Closest_Human_Distance = 0\n",
    "        self.goal_reached = 0\n",
    "        self.timeout = 0\n",
    "\n",
    "    # def _init_static_obstacles(self):\n",
    "    #     obstacles = []\n",
    "    #     for _ in range(self.num_static_obstacles):\n",
    "    #         obstacles.append(np.random.uniform(0, self.grid_size, size=2).round(1))\n",
    "    #     return obstacles\n",
    "    \n",
    "    # def _init_dynamic_obstacles(self):\n",
    "    #     obstacles = []\n",
    "    #     for _ in range(self.num_dynamic_obstacles):\n",
    "    #         start_pos = np.random.uniform(0, self.grid_size, size=2).round(1)\n",
    "    #         end_pos = np.random.uniform(0, self.grid_size, size=2).round(1)\n",
    "    #         path = self._compute_path(start_pos, end_pos)\n",
    "    #         obstacles.append({\n",
    "    #             'start': start_pos,\n",
    "    #             'end': end_pos,\n",
    "    #             'current': start_pos.copy(),\n",
    "    #             'angle': np.rad2deg(np.arctan2(end_pos[1] - start_pos[1], end_pos[0] - start_pos[0])),\n",
    "    #             'distance': np.linalg.norm(end_pos - start_pos),\n",
    "    #             'path': path if path else []\n",
    "    #         })\n",
    "    #     return obstacles\n",
    "\n",
    "    def _init_static_obstacles(self):\n",
    "        obstacles = []\n",
    "        for _ in range(self.num_static_obstacles):\n",
    "            # obstacles.append(np.random.uniform(0, self.grid_size, size=2).round(1))\n",
    "            obstacles.append(np.array(np.random.randint(1, self.grid_size, size=2)))\n",
    "        return obstacles\n",
    "    \n",
    "    def _init_dynamic_obstacles(self):\n",
    "        obstacles = []\n",
    "        for _ in range(self.num_dynamic_obstacles):\n",
    "            # start_pos = np.random.uniform(0, self.grid_size, size=2).round(1)\n",
    "            # end_pos = np.random.uniform(0, self.grid_size, size=2).round(1)\n",
    "            start_pos = np.array(np.random.randint(0, self.grid_size, size=2))\n",
    "            end_pos = np.array(np.random.randint(0, self.grid_size, size=2))\n",
    "            path = self._compute_path(start_pos, end_pos)\n",
    "            obstacles.append({\n",
    "                'start': start_pos,\n",
    "                'end': end_pos,\n",
    "                'current': start_pos.copy(),\n",
    "                'angle': np.rad2deg(np.arctan2(self.agent_pos[1] - start_pos[1], self.agent_pos[0] - start_pos[0])),\n",
    "                'distance': np.linalg.norm(self.agent_pos - start_pos),\n",
    "                'path': path if path else []\n",
    "            })\n",
    "        return obstacles\n",
    "\n",
    "    def _compute_path(self, start, end):\n",
    "        def heuristic(a, b):\n",
    "            return np.linalg.norm(a - b)\n",
    "\n",
    "        def a_star(start, goal):\n",
    "            open_set = []\n",
    "            heapq.heappush(open_set, (0, tuple(start)))\n",
    "            came_from = {}\n",
    "            g_score = {tuple(start): 0}\n",
    "            f_score = {tuple(start): heuristic(start, goal)}\n",
    "\n",
    "            while open_set:\n",
    "                _, current = heapq.heappop(open_set)\n",
    "                current = np.array(current)\n",
    "\n",
    "                if np.array_equal(current, goal):\n",
    "                    path = []\n",
    "                    while tuple(current) in came_from:\n",
    "                        path.append(current)\n",
    "                        current = came_from[tuple(current)]\n",
    "                    path.append(start)\n",
    "                    path.reverse()\n",
    "                    return path\n",
    "                \n",
    "                val = 0.5\n",
    "                neighbors = [current + [val, 0], current + [-val, 0], current + [0, val], current + [0, -val]]\n",
    "                neighbors = [np.clip(neighbor, 0, self.grid_size - 0.5) for neighbor in neighbors]\n",
    "\n",
    "                for neighbor in neighbors:\n",
    "                    tentative_g_score = g_score[tuple(current)] + heuristic(current, neighbor)\n",
    "                    if tuple(neighbor) not in g_score or tentative_g_score < g_score[tuple(neighbor)]:\n",
    "                        came_from[tuple(neighbor)] = current\n",
    "                        g_score[tuple(neighbor)] = tentative_g_score\n",
    "                        f_score[tuple(neighbor)] = tentative_g_score + heuristic(neighbor, goal)\n",
    "                        heapq.heappush(open_set, (f_score[tuple(neighbor)], tuple(neighbor)))\n",
    "\n",
    "            return []\n",
    "\n",
    "        return a_star(start, end)\n",
    "    \n",
    "    # def step(self, action):\n",
    "    #     self.current_step += 1\n",
    "    #     previous_agent_pos = self.agent_pos.copy()\n",
    "    #     print(f\"Agent's Current Position {previous_agent_pos}\")\n",
    "        \n",
    "    #     if action == 0:\n",
    "    #         self.agent_pos[1] += 0.1\n",
    "    #         print(\"Action Taken : Up\")\n",
    "    #     elif action == 1:\n",
    "    #         self.agent_pos[1] -= 0.1\n",
    "    #         print(\"Action Taken : Down\")\n",
    "    #     elif action == 2:\n",
    "    #         self.agent_pos[0] -= 0.1\n",
    "    #         print(\"Action Taken : Left\")\n",
    "    #     elif action == 3:\n",
    "    #         self.agent_pos[0] += 0.1\n",
    "    #         print(\"Action Taken : Right\")\n",
    "    #     elif action == 4:\n",
    "    #         self.agent_pos += [0.1, 0.1]\n",
    "    #         print(\"Action Taken : Right-Up\")\n",
    "    #     elif action == 5:\n",
    "    #         self.agent_pos += [-0.1, 0.1]\n",
    "    #         print(\"Action Taken : Left-Up\")\n",
    "    #     elif action == 6:\n",
    "    #         self.agent_pos += [0.1, -0.1]\n",
    "    #         print(\"Action Taken : Right-Down\")\n",
    "    #     elif action == 7:\n",
    "    #         self.agent_pos += [-0.1, -0.1]\n",
    "    #         print(\"Action Taken : Left-Down\")\n",
    "    #     elif action == 8:\n",
    "    #         print(\"Action Taken : Stay Still!\")\n",
    "    #         pass\n",
    "        \n",
    "    #     self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size - 0.5)\n",
    "    #     print(f\"Agent's New Position {self.agent_pos}\")\n",
    "        \n",
    "    #     for obstacle in self.dynamic_obstacles:\n",
    "    #         if len(obstacle['path']) > 1:\n",
    "    #             obstacle['current'] = obstacle['path'].pop(0)\n",
    "    #         else:\n",
    "    #             obstacle['path'] = self._compute_path(obstacle['start'], obstacle['end'])\n",
    "        \n",
    "    #     reward = self._compute_reward(previous_agent_pos)\n",
    "    #     print(f\"Reward Obtain : {reward}\")\n",
    "    #     done = self._is_done()\n",
    "    #     if self.current_step >= self.max_steps:\n",
    "    #         done = True\n",
    "    #         reward -= 50\n",
    "    #     truncated = False\n",
    "        \n",
    "    #     return self._get_obs(), reward, done, truncated, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        previous_agent_pos = self.agent_pos.copy()\n",
    "        print(f\"Agent's Current Position {previous_agent_pos}\")\n",
    "        print(f\"Goal Position : {self.goal_pos}\")\n",
    "        \n",
    "        if action == 0:\n",
    "            self.agent_pos[1] += 1\n",
    "            print(f\"Action Taken : {action} - Up\")\n",
    "        elif action == 1:\n",
    "            self.agent_pos[1] -= 1\n",
    "            print(f\"Action Taken : {action} - Down\")\n",
    "        elif action == 2:\n",
    "            self.agent_pos[0] -= 1\n",
    "            print(f\"Action Taken : {action} - Left\")\n",
    "        elif action == 3:\n",
    "            self.agent_pos[0] += 1\n",
    "            print(f\"Action Taken : {action} - Right\")\n",
    "        elif action == 4:\n",
    "            self.agent_pos += [1, 1]\n",
    "            print(f\"Action Taken : {action} - Right-Up\")\n",
    "        elif action == 5:\n",
    "            self.agent_pos += [-1, 1]\n",
    "            print(f\"Action Taken : {action} - Left-Up\")\n",
    "        elif action == 6:\n",
    "            self.agent_pos += [1, -1]\n",
    "            print(f\"Action Taken : {action} - Right-Down\")\n",
    "        elif action == 7:\n",
    "            self.agent_pos += [-1, -1]\n",
    "            print(f\"Action Taken : {action} - Left-Down\")\n",
    "        elif action == 8:\n",
    "            print(f\"Action Taken : {action} - Stay Still!\")\n",
    "            self.stalled_time += 1\n",
    "            pass\n",
    "        \n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size - 0.5)\n",
    "        print(f\"Agent's New Position {self.agent_pos}\")\n",
    "        print(f\"Steps taken : {self.current_step}\")\n",
    "        \n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            if len(obstacle['path']) > 1:\n",
    "                obstacle['current'] = obstacle['path'].pop(0)\n",
    "            else:\n",
    "                obstacle['path'] = self._compute_path(obstacle['start'], obstacle['end'])\n",
    "        \n",
    "        reward = self._compute_reward(previous_agent_pos)\n",
    "        \n",
    "        print(f\"#####################################\")\n",
    "        print(f\"Reward Obtained : {reward}\")\n",
    "        print(f\"#####################################\")\n",
    "        \n",
    "        self.total_reward += reward\n",
    "        \n",
    "        wandb.log({\"Reward\" : reward, \"Total_Reward\" : self.total_reward})\n",
    "        \n",
    "        self.done = self._is_done()\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.done = True\n",
    "            reward -= 250\n",
    "            self.timeout += 1\n",
    "            wandb.log({\"TimeOut\" : self.timeout})\n",
    "            \n",
    "            # self.reset()\n",
    "        truncated = False\n",
    "        \n",
    "        return self._get_obs(), reward, self.done, truncated, {}\n",
    "    \n",
    "    # def _compute_reward(self, previous_agent_pos):\n",
    "    #     previous_dist_to_goal = np.linalg.norm(previous_agent_pos - self.goal_pos)\n",
    "    #     current_dist_to_goal = np.linalg.norm(self.agent_pos - self.goal_pos)\n",
    "    #     reward = (current_dist_to_goal - previous_dist_to_goal) * self.dist_factor\n",
    "        \n",
    "    #     # Check for collisions with obstacles and boundaries\n",
    "    #     reward_1 = 0\n",
    "    #     for obs in self.dynamic_obstacles:\n",
    "    #         angle = np.rad2deg(np.arctan2(obs['current'][1] - self.agent_pos[1], obs['current'][0] - self.agent_pos[0]))\n",
    "    #         distance = np.linalg.norm(obs['current'] - self.agent_pos)\n",
    "    #         # Update reward based on obstacle proximity\n",
    "    #         # reward -= distance*self.dist_factor  #\n",
    "    #         if distance < 0.5:  # Define a threshold for collision\n",
    "    #             reward_1 -= 10*(1/distance)  # Penalty for colliding with a dynamic obstacle\n",
    "    #         if distance == 0:\n",
    "    #             reward_1 -= 125\n",
    "    #         if any(np.array_equal(self.agent_pos, obs) for obs in self.dynamic_obstacles):\n",
    "    #             reward_1 -= 15  # Penalty for colliding with a dynamic\n",
    "        \n",
    "    #     if any(np.array_equal(self.agent_pos, obs) for obs in self.static_obstacles):\n",
    "    #         reward -= 15  # Penalty for colliding with a static obstacle\n",
    "    #     if np.any(self.agent_pos == 0) or np.any(self.agent_pos >= self.grid_size - 0.5):\n",
    "    #         reward -= 5  # Penalty for hitting the wall/boundary\n",
    "        \n",
    "    #     reward -= 1\n",
    "    #     return reward\n",
    "    \n",
    "    def _compute_reward(self, previous_agent_pos):\n",
    "        reward_c = 0\n",
    "        \n",
    "        if self.safe:\n",
    "            self.min_time_to_collision += 1\n",
    "        \n",
    "        previous_dist_to_goal = np.linalg.norm(previous_agent_pos - self.goal_pos)\n",
    "        current_dist_to_goal = np.linalg.norm(self.agent_pos - self.goal_pos)\n",
    "        del_distance = current_dist_to_goal - previous_dist_to_goal\n",
    "        \n",
    "        print(f\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\n\")\n",
    "        print(f\"Distance to the Goal : {current_dist_to_goal}\")\n",
    "        print(f\"Del_Distance : {del_distance}\")\n",
    "        \n",
    "        if (del_distance) > 0:\n",
    "            reward_c -= del_distance*self.dist_factor\n",
    "        elif (del_distance) == 0:\n",
    "            reward_c = -2\n",
    "        else:\n",
    "            reward_c += (-del_distance)*self.dist_factor*2\n",
    "            \n",
    "        print(f\"Reward from Del_Dist : {reward_c}\")\n",
    "            \n",
    "        # Check for collisions with obstacles and boundaries\n",
    "        for i, obs in enumerate(self.dynamic_obstacles):\n",
    "            distance = np.linalg.norm(obs['current'] - self.agent_pos)\n",
    "            self.human_distance += distance\n",
    "            \n",
    "            print(f\"******************************\")\n",
    "            print(f\"Obstacle #{i} Distance : {distance}\")\n",
    "            \n",
    "            if distance < 3 and distance != 0:  # Define a threshold for collision\n",
    "                self.closest_human_distance = min(self.closest_human_distance, distance)\n",
    "                self.close_call += 1\n",
    "                                \n",
    "                pen_1 = (10/distance)\n",
    "                reward_c -= pen_1  # Penalty for colliding with a dynamic obstacle\n",
    "                print(f\"Penalty Obtained : {pen_1}\")\n",
    "        \n",
    "        print(f\"Reward Post Dynamic Manouevres : {reward_c}\")\n",
    "        print(f\"******************************\")\n",
    "                \n",
    "        if any(np.array_equal(self.agent_pos, obs) for obs in self.dynamic_obstacles):\n",
    "            self.collision_count += 1\n",
    "            self.human_collision_count += 1\n",
    "            self.safe = False\n",
    "            \n",
    "            reward_c -= 25  # Penalty for colliding with a dynamic obstacle\n",
    "            print(f\"Collided with a Human!!!\")\n",
    "            print(f\"Post Human Collision Reward : {reward_c}\")\n",
    "        if any(np.array_equal(self.agent_pos, obs) for obs in self.static_obstacles):\n",
    "            self.collision_count += 1\n",
    "            self.obstacle_collision_count += 1\n",
    "            self.safe = False\n",
    "            \n",
    "            reward_c -= 15  # Penalty for colliding with a static obstacle\n",
    "            print(f\"Collided with an obstacle!!!\")\n",
    "            print(f\"Post Obstacle Collision Reward : {reward_c}\")\n",
    "        if np.any(self.agent_pos == 0) or np.any(self.agent_pos >= self.grid_size - 0.5):\n",
    "            self.collision_count += 1\n",
    "            self.wall_collision_count += 1\n",
    "            self.safe = False\n",
    "            \n",
    "            reward_c -= 15  # Penalty for hitting the wall/boundary\n",
    "            print(f\"Collided with the wall!!!\")\n",
    "            print(f\"Post Wall Collision Reward : {reward_c}\")\n",
    "        \n",
    "        reward_c -= 1\n",
    "        \n",
    "        if self._is_done():\n",
    "            reward_c += 200\n",
    "            # self.reset()\n",
    "            \n",
    "        return reward_c\n",
    "    \n",
    "    def _is_done(self):\n",
    "        return np.array_equal(self.agent_pos, self.goal_pos)\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        agent_state = np.array([self.agent_pos[0], self.agent_pos[1]], dtype=np.float32)\n",
    "        dynamic_obstacle_states = np.array([(np.rad2deg(np.arctan2(self.agent_pos[1] - ob['current'][1], self.agent_pos[0] - ob['current'][0])), np.linalg.norm(self.agent_pos - ob['current'])) for ob in self.dynamic_obstacles] + \n",
    "                                           [np.zeros(2) for _ in range(self.num_dynamic_obstacles, self.max_dynamic_obstacles)], dtype=np.float32)\n",
    "        static_obstacle_states = np.array([ob[:2] for ob in self.static_obstacles] + \n",
    "                                          [np.zeros(2) for _ in range(self.num_static_obstacles, self.max_static_obstacles)], dtype=np.float32)\n",
    "        return {\n",
    "            'agent': agent_state,\n",
    "            'dyn_obs': dynamic_obstacle_states,\n",
    "            'sta_obs': static_obstacle_states\n",
    "        }\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if not hasattr(self, 'fig'):\n",
    "            self.fig, self.ax = plt.subplots()\n",
    "            self.ax.set_xlim(0, self.grid_size)\n",
    "            self.ax.set_ylim(0, self.grid_size)\n",
    "            # Initialize markers with initial positions\n",
    "            self.agent_marker, = self.ax.plot(self.agent_pos[0], self.agent_pos[1], 'go', markersize=10)\n",
    "            self.goal_marker, = self.ax.plot(self.goal_pos[0], self.goal_pos[1], 'rx', markersize=10)\n",
    "            self.dynamic_markers = [self.ax.plot(ob['current'][0], ob['current'][1], 'mo', markersize=5)[0] for ob in self.dynamic_obstacles]\n",
    "            self.static_markers = [self.ax.plot(ob[0], ob[1], 'bs', markersize=5)[0] for ob in self.static_obstacles]\n",
    "        else:\n",
    "            # Ensure all positions are sequences\n",
    "            self.agent_marker.set_data([self.agent_pos[0]], [self.agent_pos[1]])\n",
    "            self.goal_marker.set_data([self.goal_pos[0]], [self.goal_pos[1]])\n",
    "            for marker, obstacle in zip(self.dynamic_markers, self.dynamic_obstacles):\n",
    "                marker.set_data([obstacle['current'][0]], [obstacle['current'][1]])\n",
    "            for marker, obstacle in zip(self.static_markers, self.static_obstacles):\n",
    "                marker.set_data([obstacle[0]], [obstacle[1]])\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        display(self.fig)\n",
    "        plt.pause(0.001)  # Add a short pause to update the plot\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        \n",
    "        print(f\"PRINTING & LOGGING!!!\")\n",
    "        \n",
    "        wandb.log({\"Epiosde\" : self.ep_no})  \n",
    "        self.ep_no += 1\n",
    "        \n",
    "        self.ep_human_distance = self.human_distance/self.current_step\n",
    "        self.ep_discomfort = self.close_call/self.current_step\n",
    "        self.ep_collision_rate = self.collision_count/self.current_step\n",
    "        self.ep_obstacle_collision_rate = \n",
    "\n",
    "        self.Avg_Collision_Rate = ((self.ep_no-1)*self.Avg_Collision_Rate + self.ep_collision_rate)/self.ep_no\n",
    "        self.Avg_Min_Time_To_Collision = ((self.ep_no-1)*self.Avg_Min_Time_To_Collision + self.min_time_to_collision)/self.ep_no\n",
    "        self.Avg_Wall_Collision_Rate = ((self.ep_no-1)*self.Avg_Wall_Collision_Rate + self.wall_collision_count)/self.ep_no\n",
    "        self.Avg_Obstacle_Collision_Rate = ((self.ep_no-1)*self.Avg_Obstacle_Collision_Rate + self.obstacle_collision_count)/self.ep_no\n",
    "        self.Avg_Human_Collision_Rate = ((self.ep_no-1)*self.Avg_Human_Collision_Rate + self.human_collision_count)/self.ep_no\n",
    "        self.Avg_Path_Length = ((self.ep_no-1)*self.Avg_Path_Length + self.current_step)/self.ep_no\n",
    "        self.Avg_Stalled_Time = ((self.ep_no-1)*self.Avg_Stalled_Time + self.stalled_time)/self.ep_no\n",
    "        self.Avg_Discomfort = ((self.ep_no-1)*self.Avg_Discomfort + self.ep_discomfort)/self.ep_no\n",
    "        self.Avg_Human_Distance = ((self.ep_no-1)*self.Avg_Human_Distance + self.ep_human_distance)/self.ep_no\n",
    "        self.Avg_Closest_Human_Distance = ((self.ep_no-1)*self.Avg_Closest_Human_Distance + self.closest_human_distance)/self.ep_no\n",
    "        \n",
    "        wandb.log({\"Ep_Total_Reward\" : self.total_reward, \n",
    "                   \n",
    "                   \"Ep_Collision_Count\" : self.collision_count, \"Ep_Min_Time_To_Collision\" : self.min_time_to_collision, \n",
    "                   \"Ep_Wall_Collision_Count\" : self.wall_collision_count, \"Ep_Obstacle_Collision_Count\" : self.obstacle_collision_count, \n",
    "                   \"Ep_Human_Collision_Count\" : self.human_collision_count, \"Ep_Path_Length\" : self.current_step,\n",
    "                   \"Ep_Stalled_Time\" : self.stalled_time, \"Ep_Discomfort\" : self.ep_discomfort,\n",
    "                   \"Ep_Avg_Human_Distance\" : self.ep_human_distance, \"Ep_Closest_Human_Distance\" : self.closest_human_distance,\n",
    "                   \"Ep_Close_Calls\" : self.close_call, \n",
    "                   \n",
    "                   \"Avg_Collision_Rate\" : self.Avg_Collision_Rate, \"Avg_Min_Time_To_Collision\" : self.Avg_Min_Time_To_Collision,\n",
    "                   \"Avg_Wall_Collision_Rate\" : self.Avg_Wall_Collision_Rate, \"Avg_Obstacle_Collision_Rate\" : self.Avg_Obstacle_Collision_Rate,\n",
    "                   \"Avg_Human_Collision_Rate\" : self.Avg_Human_Collision_Rate, \"Avg_Path_Length\" : self.Avg_Path_Length,\n",
    "                   \"Avg_Stalled_Time\" : self.Avg_Stalled_Time, \"Avg_Discomfort\" : self.Avg_Discomfort,\n",
    "                   \"Avg_Human_Distance\" : self.Avg_Human_Distance, \"Avg_Closest_Human_Distance\" : self.Avg_Closest_Human_Distance,                   \n",
    "                   })\n",
    "        \n",
    "        print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "        print(f\"RUN DETAILS!!! \\n\")\n",
    "        print(f\"Ep_Total_Reward : {self.total_reward} \\n\"), \n",
    "        print(f\"Ep_Collision_Count : {self.collision_count}\")\n",
    "        print(f\"Ep_Wall_Collision_Count : {self.wall_collision_count}\")\n",
    "        print(f\"Ep_Obstacle_Collision_Count : {self.obstacle_collision_count}\")\n",
    "        print(f\"Ep_Human_Collision_Count : {self.human_collision_count}\")\n",
    "        print(f\"Ep_Min_Time_To_Collision : {self.min_time_to_collision}\")\n",
    "        print(f\"Ep_Stalled_Time : {self.stalled_time}\")\n",
    "        print(f\"Ep_Avg_Human_Distance : {self.Avg_Human_Distance}\")\n",
    "        \n",
    "        print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "        print(\"Creating the new Episode\")\n",
    "        \n",
    "    # You can optionally handle the 'seed' or other kwargs if needed\n",
    "        if 'seed' in kwargs:\n",
    "            np.random.seed(kwargs['seed'])\n",
    "\n",
    "        self.agent_pos = np.random.uniform(0, self.grid_size, size=2).round(2)\n",
    "        self.goal_pos = np.random.uniform(0, self.grid_size, size=2).round(2)\n",
    "        # self.num_static_obstacles = np.random.randint(1,10)\n",
    "        # self.num_dynamic_obstacles = np.random.randint(1,21)\n",
    "        self.num_static_obstacles = 30\n",
    "        self.num_dynamic_obstacles = 20\n",
    "        self.dynamic_obstacles = self._init_dynamic_obstacles()\n",
    "        self.static_obstacles = self._init_static_obstacles()\n",
    "        self.safe = True\n",
    "        self.done = False\n",
    "        \n",
    "        # Evaluation Metrics\n",
    "        self.Avg_Success_Rate = 0\n",
    "        self.collision_count = 0\n",
    "        self.ep_collision_rate = 0\n",
    "        self.min_time_to_collision = 0\n",
    "        self.wall_collision_count = 0\n",
    "        self.obstacle_collision_count = 0\n",
    "        self.human_collision_count = 0\n",
    "\n",
    "        self.Avg_Timeout = 0\n",
    "        self.Avg_Path_Length = 0\n",
    "        self.stalled_time = 0\n",
    "\n",
    "        self.ep_discomfort = 0\n",
    "        self.ep_human_distance, self.human_distance = 0, 0\n",
    "        self.closest_human_distance = 0\n",
    "        self.Min_Closest_Human_Distance = 0\n",
    "                \n",
    "        self.close_call, self.discomfort, self.current_step = 0, 0, 1\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        print(\"*******************************************************************\\n\")\n",
    "        print(\"Initialized the environment with the following\")\n",
    "        print(\"Agent's Initial Position :\", self.agent_pos)\n",
    "        print(\"Goal Position :\", self.goal_pos)\n",
    "        print(\"Number of Static Obstacles :\", self.num_static_obstacles)\n",
    "        print(\"Static Obstacle Positions :\", self. static_obstacles)\n",
    "        print(\"Number of Dynamic Obstacles :\", self.num_dynamic_obstacles)\n",
    "        print(\"Dynamic Obstacle theta & dist :\", self. dynamic_obstacles, \"\\n\")\n",
    "        print(\"*******************************************************************\")\n",
    "        \n",
    "        return self._get_obs(), {}  # Ensure this returns a single dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class RenderCallback(BaseCallback):\n",
    "    def __init__(self, env, render_freq=1, verbose=0):\n",
    "        super(RenderCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.render_freq = render_freq\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.render_freq == 0:\n",
    "            self.env.render()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrah-m\u001b[0m (\u001b[33mrebot\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Social_Navigation\\CODE\\wandb\\run-20240625_200149-m6d1c1j9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rebot/DGSN_runs/runs/m6d1c1j9' target=\"_blank\">cosmic-snow-17</a></strong> to <a href='https://wandb.ai/rebot/DGSN_runs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rebot/DGSN_runs' target=\"_blank\">https://wandb.ai/rebot/DGSN_runs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rebot/DGSN_runs/runs/m6d1c1j9' target=\"_blank\">https://wandb.ai/rebot/DGSN_runs/runs/m6d1c1j9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINTING & LOGGING!!!\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "RUN DETAILS!!! \n",
      "\n",
      "Ep_Total_Reward : 0 \n",
      "\n",
      "Ep_Collision_Count : 0\n",
      "Ep_Wall_Collision_Count : 0\n",
      "Ep_Obstacle_Collision_Count : 0\n",
      "Ep_Human_Collision_Count : 0\n",
      "Ep_Min_Time_To_Collision : 0\n",
      "Ep_Stalled_Time : 0\n",
      "Ep_Avg_Human_Distance : 0.0\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Creating the new Episode\n",
      "*******************************************************************\n",
      "\n",
      "Initialized the environment with the following\n",
      "Agent's Initial Position : [7.13 3.63]\n",
      "Goal Position : [2.8  3.55]\n",
      "Number of Static Obstacles : 30\n",
      "Static Obstacle Positions : [array([6, 5]), array([9, 6]), array([9, 3]), array([1, 8]), array([6, 8]), array([9, 7]), array([8, 8]), array([2, 6]), array([6, 3]), array([1, 9]), array([7, 3]), array([2, 3]), array([3, 8]), array([7, 3]), array([7, 3]), array([3, 6]), array([4, 6]), array([3, 6]), array([9, 1]), array([1, 5]), array([7, 3]), array([4, 2]), array([5, 1]), array([8, 2]), array([2, 6]), array([4, 7]), array([8, 8]), array([1, 2]), array([2, 6]), array([4, 8])]\n",
      "Number of Dynamic Obstacles : 20\n",
      "Dynamic Obstacle theta & dist : [{'start': array([6, 9]), 'end': array([1, 1]), 'current': array([6, 9]), 'angle': -78.11671932117397, 'distance': 5.487604213133451, 'path': [array([6, 9]), array([6. , 8.5]), array([6., 8.]), array([6. , 7.5]), array([6., 7.]), array([6. , 6.5]), array([6., 6.]), array([5.5, 6. ]), array([5.5, 5.5]), array([5. , 5.5]), array([5., 5.]), array([4.5, 5. ]), array([4.5, 4.5]), array([4. , 4.5]), array([4., 4.]), array([3.5, 4. ]), array([3.5, 3.5]), array([3. , 3.5]), array([3., 3.]), array([2.5, 3. ]), array([2.5, 2.5]), array([2. , 2.5]), array([2., 2.]), array([1.5, 2. ]), array([1.5, 1.5]), array([1. , 1.5]), array([1., 1.])]}, {'start': array([1, 8]), 'end': array([0, 5]), 'current': array([1, 8]), 'angle': -35.48459339130833, 'distance': 7.5282003161446225, 'path': [array([1, 8]), array([1. , 7.5]), array([1., 7.]), array([1. , 6.5]), array([1., 6.]), array([0.5, 6. ]), array([0.5, 5.5]), array([0. , 5.5]), array([0., 5.])]}, {'start': array([9, 8]), 'end': array([6, 0]), 'current': array([9, 8]), 'angle': -113.1669353900662, 'distance': 4.753293594971806, 'path': [array([9, 8]), array([9. , 7.5]), array([9., 7.]), array([9. , 6.5]), array([9., 6.]), array([9. , 5.5]), array([9., 5.]), array([9. , 4.5]), array([9., 4.]), array([9. , 3.5]), array([9., 3.]), array([8.5, 3. ]), array([8.5, 2.5]), array([8. , 2.5]), array([8., 2.]), array([7.5, 2. ]), array([7.5, 1.5]), array([7. , 1.5]), array([7., 1.]), array([6.5, 1. ]), array([6.5, 0.5]), array([6. , 0.5]), array([6., 0.])]}, {'start': array([2, 0]), 'end': array([9, 3]), 'current': array([2, 0]), 'angle': 35.2833141822149, 'distance': 6.284409280115355, 'path': [array([2, 0]), array([2.5, 0. ]), array([3., 0.]), array([3.5, 0. ]), array([4., 0.]), array([4.5, 0. ]), array([5., 0.]), array([5.5, 0. ]), array([6., 0.]), array([6. , 0.5]), array([6.5, 0.5]), array([6.5, 1. ]), array([7., 1.]), array([7. , 1.5]), array([7.5, 1.5]), array([7.5, 2. ]), array([8., 2.]), array([8. , 2.5]), array([8.5, 2.5]), array([8.5, 3. ]), array([9., 3.])]}, {'start': array([8, 4]), 'end': array([6, 4]), 'current': array([8, 4]), 'angle': -156.96056402009648, 'distance': 0.9454099639838796, 'path': [array([8, 4]), array([7.5, 4. ]), array([7., 4.]), array([6.5, 4. ]), array([6., 4.])]}, {'start': array([5, 9]), 'end': array([3, 5]), 'current': array([5, 9]), 'angle': -68.36434447605963, 'distance': 5.77700614505472, 'path': [array([5, 9]), array([5. , 8.5]), array([5., 8.]), array([5. , 7.5]), array([5., 7.]), array([4.5, 7. ]), array([4.5, 6.5]), array([4. , 6.5]), array([4., 6.]), array([3.5, 6. ]), array([3.5, 5.5]), array([3. , 5.5]), array([3., 5.])]}, {'start': array([8, 7]), 'end': array([3, 0]), 'current': array([8, 7]), 'angle': -104.47543596306143, 'distance': 3.480488471464889, 'path': [array([8, 7]), array([8. , 6.5]), array([8., 6.]), array([8. , 5.5]), array([8., 5.]), array([7.5, 5. ]), array([7.5, 4.5]), array([7. , 4.5]), array([7., 4.]), array([6.5, 4. ]), array([6.5, 3.5]), array([6. , 3.5]), array([6., 3.]), array([5.5, 3. ]), array([5.5, 2.5]), array([5. , 2.5]), array([5., 2.]), array([4.5, 2. ]), array([4.5, 1.5]), array([4. , 1.5]), array([4., 1.]), array([3.5, 1. ]), array([3.5, 0.5]), array([3. , 0.5]), array([3., 0.])]}, {'start': array([2, 6]), 'end': array([0, 4]), 'current': array([2, 6]), 'angle': -24.796384831674864, 'distance': 5.650999911520084, 'path': [array([2, 6]), array([1.5, 6. ]), array([1.5, 5.5]), array([1. , 5.5]), array([1., 5.]), array([0.5, 5. ]), array([0.5, 4.5]), array([0. , 4.5]), array([0., 4.])]}, {'start': array([6, 7]), 'end': array([8, 1]), 'current': array([6, 7]), 'angle': -71.4631014990624, 'distance': 3.5544057168533816, 'path': [array([6, 7]), array([6. , 6.5]), array([6., 6.]), array([6. , 5.5]), array([6., 5.]), array([6. , 4.5]), array([6., 4.]), array([6. , 3.5]), array([6., 3.]), array([6. , 2.5]), array([6.5, 2.5]), array([6.5, 2. ]), array([7., 2.]), array([7. , 1.5]), array([7.5, 1.5]), array([7.5, 1. ]), array([8., 1.])]}, {'start': array([6, 3]), 'end': array([1, 9]), 'current': array([6, 3]), 'angle': 29.14063373339431, 'distance': 1.2937542270462343, 'path': [array([6, 3]), array([6. , 3.5]), array([6., 4.]), array([5.5, 4. ]), array([5.5, 4.5]), array([5. , 4.5]), array([5., 5.]), array([4.5, 5. ]), array([4.5, 5.5]), array([4. , 5.5]), array([4., 6.]), array([3.5, 6. ]), array([3.5, 6.5]), array([3. , 6.5]), array([3., 7.]), array([2.5, 7. ]), array([2.5, 7.5]), array([2. , 7.5]), array([2., 8.]), array([1.5, 8. ]), array([1.5, 8.5]), array([1. , 8.5]), array([1., 9.])]}, {'start': array([9, 7]), 'end': array([1, 2]), 'current': array([9, 7]), 'angle': -119.02572518039716, 'distance': 3.854062791393, 'path': [array([9, 7]), array([8.5, 7. ]), array([8., 7.]), array([7.5, 7. ]), array([7., 7.]), array([6.5, 7. ]), array([6., 7.]), array([5.5, 7. ]), array([5.5, 6.5]), array([5. , 6.5]), array([5., 6.]), array([4.5, 6. ]), array([4.5, 5.5]), array([4. , 5.5]), array([4., 5.]), array([3.5, 5. ]), array([3.5, 4.5]), array([3. , 4.5]), array([3., 4.]), array([2.5, 4. ]), array([2.5, 3.5]), array([2. , 3.5]), array([2., 3.]), array([1.5, 3. ]), array([1.5, 2.5]), array([1. , 2.5]), array([1., 2.])]}, {'start': array([4, 4]), 'end': array([4, 3]), 'current': array([4, 4]), 'angle': -6.741697293544252, 'distance': 3.1517931404202275, 'path': [array([4, 4]), array([4. , 3.5]), array([4., 3.])]}, {'start': array([4, 8]), 'end': array([0, 3]), 'current': array([4, 8]), 'angle': -54.38797651508878, 'distance': 5.37529534072315, 'path': [array([4, 8]), array([4. , 7.5]), array([4., 7.]), array([3.5, 7. ]), array([3.5, 6.5]), array([3. , 6.5]), array([3., 6.]), array([2.5, 6. ]), array([2.5, 5.5]), array([2. , 5.5]), array([2., 5.]), array([1.5, 5. ]), array([1.5, 4.5]), array([1. , 4.5]), array([1., 4.]), array([0.5, 4. ]), array([0.5, 3.5]), array([0. , 3.5]), array([0., 3.])]}, {'start': array([5, 7]), 'end': array([6, 6]), 'current': array([5, 7]), 'angle': -57.7051696900976, 'distance': 3.986702898386084, 'path': [array([5, 7]), array([5. , 6.5]), array([5.5, 6.5]), array([5.5, 6. ]), array([6., 6.])]}, {'start': array([1, 7]), 'end': array([7, 6]), 'current': array([1, 7]), 'angle': -28.800028581294814, 'distance': 6.995269830392535, 'path': [array([1, 7]), array([1.5, 7. ]), array([2., 7.]), array([2.5, 7. ]), array([3., 7.]), array([3.5, 7. ]), array([4., 7.]), array([4.5, 7. ]), array([5., 7.]), array([5.5, 7. ]), array([6., 7.]), array([6. , 6.5]), array([6.5, 6.5]), array([6.5, 6. ]), array([7., 6.])]}, {'start': array([4, 7]), 'end': array([2, 3]), 'current': array([4, 7]), 'angle': -47.114575880072664, 'distance': 4.599326037584203, 'path': [array([4, 7]), array([4. , 6.5]), array([4., 6.]), array([4. , 5.5]), array([4., 5.]), array([3.5, 5. ]), array([3.5, 4.5]), array([3. , 4.5]), array([3., 4.]), array([2.5, 4. ]), array([2.5, 3.5]), array([2. , 3.5]), array([2., 3.])]}, {'start': array([5, 2]), 'end': array([8, 5]), 'current': array([5, 2]), 'angle': 37.425320316842146, 'distance': 2.682126022393429, 'path': [array([5, 2]), array([5. , 2.5]), array([5.5, 2.5]), array([5.5, 3. ]), array([6., 3.]), array([6. , 3.5]), array([6.5, 3.5]), array([6.5, 4. ]), array([7., 4.]), array([7. , 4.5]), array([7.5, 4.5]), array([7.5, 5. ]), array([8., 5.])]}, {'start': array([8, 5]), 'end': array([8, 2]), 'current': array([8, 5]), 'angle': -122.41703750592308, 'distance': 1.6228986413205233, 'path': [array([8, 5]), array([8. , 4.5]), array([8., 4.]), array([8. , 3.5]), array([8., 3.]), array([8. , 2.5]), array([8., 2.])]}, {'start': array([1, 0]), 'end': array([3, 6]), 'current': array([1, 0]), 'angle': 30.63273019402733, 'distance': 7.124170127109543, 'path': [array([1, 0]), array([1. , 0.5]), array([1., 1.]), array([1. , 1.5]), array([1., 2.]), array([1. , 2.5]), array([1., 3.]), array([1. , 3.5]), array([1., 4.]), array([1. , 4.5]), array([1.5, 4.5]), array([1.5, 5. ]), array([2., 5.]), array([2. , 5.5]), array([2.5, 5.5]), array([2.5, 6. ]), array([3., 6.])]}, {'start': array([4, 3]), 'end': array([5, 2]), 'current': array([4, 3]), 'angle': 11.380320374129818, 'distance': 3.1927730893378565, 'path': [array([4, 3]), array([4. , 2.5]), array([4.5, 2.5]), array([4.5, 2. ]), array([5., 2.])]}] \n",
      "\n",
      "*******************************************************************\n"
     ]
    }
   ],
   "source": [
    "env = DGSN_Env()\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Train','Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('agent': Box(0.0, 10.0, (2,), float32), 'dyn_obs': Box(0.0, 10.0, (20, 2), float32), 'sta_obs': Box(0.0, 10.0, (30, 2), float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('agent', array([6.1189165, 9.152051 ], dtype=float32)),\n",
       "             ('dyn_obs',\n",
       "              array([[5.489024 , 4.5442944],\n",
       "                     [7.904686 , 5.137048 ],\n",
       "                     [6.7361217, 1.3283217],\n",
       "                     [4.3016987, 5.2996755],\n",
       "                     [7.603074 , 8.418114 ],\n",
       "                     [2.1132405, 2.7645314],\n",
       "                     [6.678677 , 3.2864237],\n",
       "                     [3.0539863, 4.2220383],\n",
       "                     [7.252383 , 2.5608544],\n",
       "                     [8.903012 , 8.056939 ],\n",
       "                     [9.547631 , 9.293203 ],\n",
       "                     [4.4504743, 4.013942 ],\n",
       "                     [7.179473 , 9.210715 ],\n",
       "                     [2.8742006, 5.885038 ],\n",
       "                     [9.251681 , 4.139462 ],\n",
       "                     [1.868111 , 0.4401924],\n",
       "                     [4.3139577, 1.4352412],\n",
       "                     [5.565343 , 3.8423066],\n",
       "                     [6.813653 , 8.491708 ],\n",
       "                     [9.677646 , 3.301888 ]], dtype=float32)),\n",
       "             ('sta_obs',\n",
       "              array([[5.0015273 , 3.4799752 ],\n",
       "                     [0.67129856, 0.52848405],\n",
       "                     [3.7613933 , 0.62703633],\n",
       "                     [1.9766016 , 5.7151213 ],\n",
       "                     [0.3545391 , 6.5155296 ],\n",
       "                     [0.33329704, 7.067453  ],\n",
       "                     [3.521219  , 2.8280182 ],\n",
       "                     [5.5663624 , 8.603152  ],\n",
       "                     [1.131609  , 9.677512  ],\n",
       "                     [9.0974045 , 8.748243  ],\n",
       "                     [1.7952914 , 6.804653  ],\n",
       "                     [6.675917  , 9.673912  ],\n",
       "                     [2.973747  , 7.0876117 ],\n",
       "                     [0.8760279 , 7.0718455 ],\n",
       "                     [6.5832253 , 2.6811216 ],\n",
       "                     [4.6110187 , 8.317426  ],\n",
       "                     [4.0881495 , 0.21002954],\n",
       "                     [8.941552  , 6.9863644 ],\n",
       "                     [0.58648825, 1.9226629 ],\n",
       "                     [7.0427427 , 9.214138  ],\n",
       "                     [2.3598645 , 1.1422393 ],\n",
       "                     [6.3330407 , 8.024474  ],\n",
       "                     [6.761061  , 9.004697  ],\n",
       "                     [4.889943  , 7.746519  ],\n",
       "                     [7.90395   , 4.6240196 ],\n",
       "                     [6.7288647 , 0.07845974],\n",
       "                     [0.7924624 , 8.90263   ],\n",
       "                     [2.3576124 , 6.837452  ],\n",
       "                     [7.3538637 , 8.798451  ],\n",
       "                     [6.825963  , 4.4642735 ]], dtype=float32))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'env' is your environment and 'model' is your RL model\n",
    "render_callback = RenderCallback(env, render_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MultiInputPolicy\", env, verbose=1, tensorboard_log=log_path, device='cuda', n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's Current Position [0.  9.5]\n",
      "Goal Position : [4.18 5.33]\n",
      "Action Taken : 7 - Left-Down\n",
      "Agent's New Position [0.  8.5]\n",
      "Steps taken : 616\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \n",
      "\n",
      "Distance to the Goal : 5.246074723066761\n",
      "Del_Distance : -0.6582711340117342\n",
      "Reward from Del_Dist : 2.633084536046937\n",
      "******************************\n",
      "Obstacle #0 Distance : 7.905694150420948\n",
      "******************************\n",
      "Obstacle #1 Distance : 9.656603957913983\n",
      "******************************\n",
      "Obstacle #2 Distance : 6.5\n",
      "******************************\n",
      "Obstacle #3 Distance : 8.74642784226795\n",
      "******************************\n",
      "Obstacle #4 Distance : 2.0\n",
      "Penalty Obtained : 5.0\n",
      "******************************\n",
      "Obstacle #5 Distance : 6.020797289396148\n",
      "******************************\n",
      "Obstacle #6 Distance : 2.0615528128088303\n",
      "Penalty Obtained : 4.85071250072666\n",
      "******************************\n",
      "Obstacle #7 Distance : 7.106335201775948\n",
      "******************************\n",
      "Obstacle #8 Distance : 8.139410298049853\n",
      "******************************\n",
      "Obstacle #9 Distance : 6.800735254367722\n",
      "******************************\n",
      "Obstacle #10 Distance : 5.5901699437494745\n",
      "******************************\n",
      "Obstacle #11 Distance : 5.830951894845301\n",
      "******************************\n",
      "Obstacle #12 Distance : 8.139410298049853\n",
      "******************************\n",
      "Obstacle #13 Distance : 6.800735254367722\n",
      "******************************\n",
      "Obstacle #14 Distance : 5.1478150704935\n",
      "******************************\n",
      "Obstacle #15 Distance : 5.0\n",
      "******************************\n",
      "Obstacle #16 Distance : 8.06225774829855\n",
      "******************************\n",
      "Obstacle #17 Distance : 6.324555320336759\n",
      "******************************\n",
      "Obstacle #18 Distance : 4.301162633521313\n",
      "******************************\n",
      "Obstacle #19 Distance : 8.514693182963201\n",
      "Reward Post Dynamic Manouevres : -7.217627964679723\n",
      "******************************\n",
      "Collided with the wall!!!\n",
      "Post Wall Collision Reward : -22.21762796467972\n",
      "#####################################\n",
      "Reward Obtained : -23.21762796467972\n",
      "#####################################\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000000, callback=render_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
