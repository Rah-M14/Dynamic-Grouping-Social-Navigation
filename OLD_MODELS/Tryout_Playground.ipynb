{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "import heapq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as patches\n",
    "# import matplotlib.animation as animation\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 4.]\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "l = np.array(np.random.randint(0,5, size=2)).astype('float')\n",
    "print(l)\n",
    "print(type(l[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGSN_Env(gym.Env):\n",
    "    def __init__(self, grid_size=10, max_static_obstacles=30, max_dynamic_obstacles=20, max_steps=500):\n",
    "        super(DGSN_Env, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.boundary_threshold = 0.5\n",
    "        self.max_static_obstacles = max_static_obstacles\n",
    "        self.max_dynamic_obstacles = max_dynamic_obstacles\n",
    "        # self.num_static_obstacles = np.random.randint(1,10)\n",
    "        # self.num_dynamic_obstacles = np.random.randint(1,21)\n",
    "        # self.agent_pos = np.array(np.random.uniform(0, self.grid_size, size=2).round(1))\n",
    "        # self.goal_pos = np.array(np.random.uniform(0, self.grid_size, size=2).round(1))\n",
    "        self.num_static_obstacles = 30\n",
    "        self.num_dynamic_obstacles = 20\n",
    "\n",
    "        self.agent_pos = np.array(np.random.randint(0, self.grid_size, size=2)).astype('float')\n",
    "        self.goal_pos = np.array(np.random.randint(0, self.grid_size, size=2)).astype('float')\n",
    "        \n",
    "        self.dynamic_obstacles = self._init_dynamic_obstacles()\n",
    "        self.static_obstacles = self._init_static_obstacles()\n",
    "        \n",
    "        self.close_call, self.discomfort, self.current_step, self.ep_no = 0, 0, 1, 0\n",
    "        self.dist_factor = 4\n",
    "        self.thresh = 1.5\n",
    "        self.max_steps = max_steps\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        self.action_space = spaces.Discrete(9)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'agent': spaces.Box(low=0, high=grid_size, shape=(2,), dtype=np.float32),\n",
    "            'dyn_obs': spaces.Box(low=0, high=grid_size, shape=(self.max_dynamic_obstacles, 2), dtype=np.float64),\n",
    "            'sta_obs': spaces.Box(low=0, high=grid_size, shape=(self.max_static_obstacles, 2), dtype=np.float64)\n",
    "        })\n",
    "        \n",
    "        # wandb.login()\n",
    "        # wandb.init(project='DGSN_runs')\n",
    "        \n",
    "        self.safe = True\n",
    "        \n",
    "        # Evaluation Metrics\n",
    "        self.Avg_Success_Rate = 0\n",
    "        self.Avg_Collision_Rate, self.ep_collision_rate, self.collision_count = 0, 0, 0\n",
    "        self.Avg_Min_Time_To_Collision, self.min_time_to_collision = 0, 0\n",
    "        self.Avg_Wall_Collision_Rate, self.ep_wall_collision_rate, self.wall_collision_count = 0, 0, 0\n",
    "        self.Avg_Obstacle_Collision_Rate, self.ep_obstacle_collision_rate, self.obstacle_collision_count = 0, 0, 0\n",
    "        self.Avg_Human_Collision_Rate, self.ep_human_collision_rate, self.human_collision_count = 0, 0, 0\n",
    "\n",
    "        self.Avg_Timeout = 0\n",
    "        self.Avg_Path_Length = 0\n",
    "        self.Avg_Stalled_Time, self.stalled_time = 0, 0\n",
    "\n",
    "        self.Avg_Group_Inhibition_Rate, self.ep_group_inhibition_rate, self.group_inhibition = 0, 0, 0\n",
    "        self.Avg_Discomfort, self.ep_discomfort = 0, 0\n",
    "        self.Avg_Human_Distance, self.ep_human_distance, self.human_distance = 0, 0, 0\n",
    "        self.Avg_Closest_Human_Distance, self.closest_human_distance = 0, 0\n",
    "        self.Min_Closest_Human_Distance = 0\n",
    "        self.goal_reached = 0\n",
    "        self.timeout = 0\n",
    "\n",
    "    def _init_static_obstacles(self):\n",
    "        obstacles = []\n",
    "        for _ in range(self.num_static_obstacles):\n",
    "            # obstacles.append(np.random.uniform(0, self.grid_size, size=2).round(1))\n",
    "            obstacles.append((np.array(np.random.randint(1, self.grid_size, size=2))).astype('float'))\n",
    "        return obstacles\n",
    "    \n",
    "    def _init_dynamic_obstacles(self):\n",
    "        obstacles = []\n",
    "        for _ in range(self.num_dynamic_obstacles):\n",
    "            # start_pos = np.random.uniform(0, self.grid_size, size=2).round(1)\n",
    "            # end_pos = np.random.uniform(0, self.grid_size, size=2).round(1)\n",
    "            start_pos = np.array(np.random.randint(0, self.grid_size, size=2)).astype('float')\n",
    "            end_pos = np.array(np.random.randint(0, self.grid_size, size=2)).astype('float')\n",
    "            path = self._compute_path(start_pos, end_pos)\n",
    "            obstacles.append({\n",
    "                'start': start_pos,\n",
    "                'end': end_pos,\n",
    "                'current': start_pos.copy(),\n",
    "                'angle': np.rad2deg(np.arctan2(self.agent_pos[1] - start_pos[1], self.agent_pos[0] - start_pos[0])).astype('float'),\n",
    "                'distance': np.linalg.norm(self.agent_pos - start_pos).astype('float'),\n",
    "                'path': path if path else []\n",
    "            })\n",
    "        return obstacles\n",
    "\n",
    "    def _compute_path(self, start, end):\n",
    "        def heuristic(a, b):\n",
    "            return np.linalg.norm(a - b)\n",
    "\n",
    "        def a_star(start, goal):\n",
    "            open_set = []\n",
    "            heapq.heappush(open_set, (0, tuple(start)))\n",
    "            came_from = {}\n",
    "            g_score = {tuple(start): 0}\n",
    "            f_score = {tuple(start): heuristic(start, goal)}\n",
    "\n",
    "            while open_set:\n",
    "                _, current = heapq.heappop(open_set)\n",
    "                current = np.array(current)\n",
    "\n",
    "                if np.array_equal(current, goal):\n",
    "                    path = []\n",
    "                    while tuple(current) in came_from:\n",
    "                        path.append(current)\n",
    "                        current = came_from[tuple(current)]\n",
    "                    path.append(start)\n",
    "                    path.reverse()\n",
    "                    return path\n",
    "                \n",
    "                val = 0.1\n",
    "                neighbors = [current + [val, 0], current + [-val, 0], current + [0, val], current + [0, -val]]\n",
    "                neighbors = [np.clip(neighbor, 0, self.grid_size - 0.5) for neighbor in neighbors]\n",
    "\n",
    "                for neighbor in neighbors:\n",
    "                    tentative_g_score = g_score[tuple(current)] + heuristic(current, neighbor)\n",
    "                    if tuple(neighbor) not in g_score or tentative_g_score < g_score[tuple(neighbor)]:\n",
    "                        came_from[tuple(neighbor)] = current\n",
    "                        g_score[tuple(neighbor)] = tentative_g_score\n",
    "                        f_score[tuple(neighbor)] = tentative_g_score + heuristic(neighbor, goal)\n",
    "                        heapq.heappush(open_set, (f_score[tuple(neighbor)], tuple(neighbor)))\n",
    "            return []\n",
    "\n",
    "        return a_star(start, end)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        previous_agent_pos = self.agent_pos.copy()\n",
    "        print(f\"Agent's Current Position {previous_agent_pos}\")\n",
    "        print(f\"Goal Position : {self.goal_pos}\")\n",
    "        \n",
    "        vel = 0.2\n",
    "        \n",
    "        if action == 0:\n",
    "            self.agent_pos[1] += vel\n",
    "            print(f\"Action Taken : {action} - Up\")\n",
    "        elif action == 1:\n",
    "            self.agent_pos[1] -= vel\n",
    "            print(f\"Action Taken : {action} - Down\")\n",
    "        elif action == 2:\n",
    "            self.agent_pos[0] -= vel\n",
    "            print(f\"Action Taken : {action} - Left\")\n",
    "        elif action == 3:\n",
    "            self.agent_pos[0] += vel\n",
    "            print(f\"Action Taken : {action} - Right\")\n",
    "        elif action == 4:\n",
    "            self.agent_pos += [vel, vel]\n",
    "            print(f\"Action Taken : {action} - Right-Up\")\n",
    "        elif action == 5:\n",
    "            self.agent_pos += [-vel, vel]\n",
    "            print(f\"Action Taken : {action} - Left-Up\")\n",
    "        elif action == 6:\n",
    "            self.agent_pos += [vel, -vel]\n",
    "            print(f\"Action Taken : {action} - Right-Down\")\n",
    "        elif action == 7:\n",
    "            self.agent_pos += [-vel, -vel]\n",
    "            print(f\"Action Taken : {action} - Left-Down\")\n",
    "        elif action == 8:\n",
    "            print(f\"Action Taken : {action} - Stay Still!\")\n",
    "            self.stalled_time += 1\n",
    "            pass\n",
    "        \n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size - 0.5)\n",
    "        print(f\"Agent's New Position {self.agent_pos}\")\n",
    "        print(f\"Steps taken : {self.current_step}\")\n",
    "        \n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            if len(obstacle['path']) > 1:\n",
    "                obstacle['current'] = obstacle['path'].pop(0)\n",
    "            else:\n",
    "                obstacle['path'] = self._compute_path(obstacle['start'], obstacle['end'])\n",
    "        \n",
    "        reward = self._compute_reward(previous_agent_pos)\n",
    "        \n",
    "        print(f\"#####################################\")\n",
    "        print(f\"Reward Obtained : {reward}\")\n",
    "        print(f\"#####################################\")\n",
    "        \n",
    "        self.total_reward += reward\n",
    "        \n",
    "        # wandb.log({\"Reward\" : reward, \"Total_Reward\" : self.total_reward})\n",
    "        \n",
    "        self.done = self._is_done()\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.done = True\n",
    "            reward -= 2500\n",
    "            self.timeout += 1\n",
    "            # wandb.log({\"TimeOut\" : self.timeout})\n",
    "            \n",
    "            # self.reset()\n",
    "        truncated = False\n",
    "        \n",
    "        return self._get_obs(), reward, self.done, truncated, {}\n",
    "    \n",
    "    def _compute_reward(self, previous_agent_pos):\n",
    "        reward_c = 0\n",
    "        \n",
    "        if self.safe:\n",
    "            self.min_time_to_collision += 1\n",
    "        \n",
    "        previous_dist_to_goal = np.linalg.norm(previous_agent_pos - self.goal_pos)\n",
    "        current_dist_to_goal = np.linalg.norm(self.agent_pos - self.goal_pos)\n",
    "        del_distance = current_dist_to_goal - previous_dist_to_goal\n",
    "        \n",
    "        print(f\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\n\")\n",
    "        print(f\"Distance to the Goal : {current_dist_to_goal}\")\n",
    "        print(f\"Del_Distance : {del_distance}\")\n",
    "        \n",
    "        if (del_distance) > 0:\n",
    "            reward_c -= del_distance*self.dist_factor\n",
    "        elif (del_distance) == 0:\n",
    "            reward_c = -2\n",
    "        else:\n",
    "            reward_c += (-del_distance)*self.dist_factor*2\n",
    "            \n",
    "        print(f\"Reward from Del_Dist : {reward_c}\")\n",
    "            \n",
    "        # Check for collisions with obstacles and boundaries\n",
    "        for i, obs in enumerate(self.dynamic_obstacles):\n",
    "            distance = np.linalg.norm(obs['current'] - self.agent_pos)\n",
    "            self.human_distance += distance\n",
    "            \n",
    "            print(f\"******************************\")\n",
    "            print(f\"Obstacle #{i} Distance : {distance}\")\n",
    "            \n",
    "            if distance < 3 and distance != 0:  # Define a threshold for collision\n",
    "                self.closest_human_distance = min(self.closest_human_distance, distance)\n",
    "                self.close_call += 1\n",
    "                                \n",
    "                pen_1 = (10/distance)\n",
    "                reward_c -= pen_1  # Penalty for colliding with a dynamic obstacle\n",
    "                print(f\"Penalty Obtained : {pen_1}\")\n",
    "        \n",
    "        print(f\"Reward Post Dynamic Manouevres : {reward_c}\")\n",
    "        print(f\"******************************\")\n",
    "        \n",
    "        grouped_obstacles = self._group_dynamic_obstacles()\n",
    "        for j, group in enumerate(grouped_obstacles):\n",
    "            group_center = np.mean(group, axis=0)\n",
    "            if np.linalg.norm(self.agent_pos - group_center) < self.thresh:                \n",
    "                reward_c -= 50\n",
    "                \n",
    "                \n",
    "                \n",
    "                self.discomfort += 1\n",
    "                wandb.log({\"Discomfort\": self.discomfort})\n",
    "\n",
    "                \n",
    "        if any(np.array_equal(self.agent_pos, obs) for obs in self.dynamic_obstacles):\n",
    "            self.collision_count += 1\n",
    "            self.human_collision_count += 1\n",
    "            self.safe = False\n",
    "            \n",
    "            reward_c -= 30  # Penalty for colliding with a dynamic obstacle\n",
    "            print(f\"Collided with a Human!!!\")\n",
    "            print(f\"Post Human Collision Reward : {reward_c}\")\n",
    "        if any(np.array_equal(self.agent_pos, obs) for obs in self.static_obstacles):\n",
    "            self.collision_count += 1\n",
    "            self.obstacle_collision_count += 1\n",
    "            self.safe = False\n",
    "            \n",
    "            reward_c -= 20  # Penalty for colliding with a static obstacle\n",
    "            print(f\"Collided with an obstacle!!!\")\n",
    "            print(f\"Post Obstacle Collision Reward : {reward_c}\")\n",
    "        \n",
    "        if np.any(self.agent_pos <= self.boundary_threshold) or np.any(self.agent_pos >= self.grid_size - self.boundary_threshold):\n",
    "            reward_c -= -15 # Penalty for travelling close to the boundary\n",
    "            \n",
    "            print(f\"Pretty Close to the Booundary!!!\")\n",
    "            print(f\"Post Boundary Penalty Reward : {reward_c}\")\n",
    "\n",
    "        if np.any(self.agent_pos == 0) or np.any(self.agent_pos >= self.grid_size):\n",
    "            self.collision_count += 1\n",
    "            self.wall_collision_count += 1\n",
    "            self.safe = False\n",
    "            \n",
    "            reward_c -= 20  # Penalty for hitting the wall/boundary\n",
    "            print(f\"Collided with the wall!!!\")\n",
    "            print(f\"Post Wall Collision Reward : {reward_c}\")\n",
    "        \n",
    "        reward_c -= 1\n",
    "        \n",
    "        if self._is_done():\n",
    "            reward_c += 3000\n",
    "            print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \\n\")\n",
    "            print(f\"Goal Reached!!!!!\")\n",
    "            print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \\n\")\n",
    "            \n",
    "            self.goal_reached += 1\n",
    "            # wandb.log({'Goal_Reached' : self.goal_reached})\n",
    "            \n",
    "            # self.reset()\n",
    "            \n",
    "        return reward_c\n",
    "    \n",
    "    def _is_done(self):\n",
    "        return np.array_equal(self.agent_pos, self.goal_pos)\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        agent_state = np.array([self.agent_pos[0], self.agent_pos[1]], dtype=np.float32)\n",
    "        dynamic_obstacle_states = np.array([(np.rad2deg(np.arctan2(self.agent_pos[1] - ob['current'][1], self.agent_pos[0] - ob['current'][0])), np.linalg.norm(self.agent_pos - ob['current'])) for ob in self.dynamic_obstacles] + \n",
    "                                           [np.zeros(2) for _ in range(self.num_dynamic_obstacles, self.max_dynamic_obstacles)], dtype=np.float32)\n",
    "        static_obstacle_states = np.array([ob[:2] for ob in self.static_obstacles] + \n",
    "                                          [np.zeros(2) for _ in range(self.num_static_obstacles, self.max_static_obstacles)], dtype=np.float32)\n",
    "        return {\n",
    "            'agent': agent_state,\n",
    "            'dyn_obs': dynamic_obstacle_states,\n",
    "            'sta_obs': static_obstacle_states\n",
    "        }\n",
    "        \n",
    "    def _group_dynamic_obstacles(self):\n",
    "        positions = np.array([obs['current'] for obs in self.dynamic_obstacles])\n",
    "        clustering = DBSCAN(eps=0.3, min_samples=2).fit(positions)\n",
    "        labels = clustering.labels_\n",
    "\n",
    "        grouped_obstacles = []\n",
    "        for label in set(labels):\n",
    "            if label != -1:  # Ignore noise points\n",
    "                group = positions[labels == label]\n",
    "                grouped_obstacles.append(group)\n",
    "\n",
    "        return grouped_obstacles\n",
    "\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if not hasattr(self, 'fig'):\n",
    "            self.fig, self.ax = plt.subplots()\n",
    "            self.ax.set_xlim(0, self.grid_size)\n",
    "            self.ax.set_ylim(0, self.grid_size)\n",
    "            # Initialize markers with initial positions\n",
    "            self.agent_marker, = self.ax.plot(self.agent_pos[0], self.agent_pos[1], 'go', markersize=10)\n",
    "            self.goal_marker, = self.ax.plot(self.goal_pos[0], self.goal_pos[1], 'rx', markersize=10)\n",
    "            self.dynamic_markers = [self.ax.plot(ob['current'][0], ob['current'][1], 'mo', markersize=5)[0] for ob in self.dynamic_obstacles]\n",
    "            self.static_markers = [self.ax.plot(ob[0], ob[1], 'bs', markersize=5)[0] for ob in self.static_obstacles]\n",
    "        else:\n",
    "            # Ensure all positions are sequences\n",
    "            self.agent_marker.set_data([self.agent_pos[0]], [self.agent_pos[1]])\n",
    "            self.goal_marker.set_data([self.goal_pos[0]], [self.goal_pos[1]])\n",
    "            for marker, obstacle in zip(self.dynamic_markers, self.dynamic_obstacles):\n",
    "                marker.set_data([obstacle['current'][0]], [obstacle['current'][1]])\n",
    "            for marker, obstacle in zip(self.static_markers, self.static_obstacles):\n",
    "                marker.set_data([obstacle[0]], [obstacle[1]])\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        display(self.fig)\n",
    "        plt.pause(0.001)  # Add a short pause to update the plot\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        \n",
    "        print(f\"PRINTING & LOGGING!!!\")\n",
    "        \n",
    "        # wandb.log({\"Epiosde\" : self.ep_no})  \n",
    "        self.ep_no += 1\n",
    "        \n",
    "        self.ep_human_distance = self.human_distance/(self.num_dynamic_obstacles*self.current_step)\n",
    "        self.ep_discomfort = self.close_call/self.current_step\n",
    "        self.ep_collision_rate = self.collision_count/self.current_step\n",
    "        self.ep_wall_collision_rate = self.wall_collision_count/self.current_step\n",
    "        self.ep_obstacle_collision_rate = self.obstacle_collision_count/self.current_step\n",
    "        self.ep_human_collision_rate = self.human_collision_count/self.current_step\n",
    "        self.ep_group_inhibition_rate = self.group_inhibition/self.current_step\n",
    "\n",
    "        self.Avg_Collision_Rate = ((self.ep_no-1)*self.Avg_Collision_Rate + self.ep_collision_rate)/self.ep_no\n",
    "        self.Avg_Min_Time_To_Collision = ((self.ep_no-1)*self.Avg_Min_Time_To_Collision + self.min_time_to_collision)/self.ep_no\n",
    "        self.Avg_Wall_Collision_Rate = ((self.ep_no-1)*self.Avg_Wall_Collision_Rate + self.ep_wall_collision_rate)/self.ep_no\n",
    "        self.Avg_Obstacle_Collision_Rate = ((self.ep_no-1)*self.Avg_Obstacle_Collision_Rate + self.ep_obstacle_collision_rate)/self.ep_no\n",
    "        self.Avg_Human_Collision_Rate = ((self.ep_no-1)*self.Avg_Human_Collision_Rate + self.ep_human_collision_rate)/self.ep_no\n",
    "        self.Avg_Path_Length = ((self.ep_no-1)*self.Avg_Path_Length + self.current_step)/self.ep_no\n",
    "        self.Avg_Stalled_Time = ((self.ep_no-1)*self.Avg_Stalled_Time + self.stalled_time)/self.ep_no\n",
    "        self.Avg_Discomfort = ((self.ep_no-1)*self.Avg_Discomfort + self.ep_discomfort)/self.ep_no\n",
    "        self.Avg_Human_Distance = ((self.ep_no-1)*self.Avg_Human_Distance + self.ep_human_distance)/self.ep_no\n",
    "        self.Avg_Closest_Human_Distance = ((self.ep_no-1)*self.Avg_Closest_Human_Distance + self.closest_human_distance)/self.ep_no\n",
    "        self.Avg_Group_Inhibition_Rate = ((self.ep_no-1)*self.Avg_Group_Inhibition_Rate + self.ep_group_inhibition_rate)/self.ep_no\n",
    "        \n",
    "        '''\n",
    "        wandb.log({\"Ep_Total_Reward\" : self.total_reward, \n",
    "                   \n",
    "                   \"Ep_Collision_Count\" : self.collision_count, \"Ep_Min_Time_To_Collision\" : self.min_time_to_collision, \n",
    "                   \"Ep_Wall_Collision_Count\" : self.wall_collision_count, \"Ep_Obstacle_Collision_Count\" : self.obstacle_collision_count, \n",
    "                   \"Ep_Human_Collision_Count\" : self.human_collision_count, \"Ep_Collision_Rate\" : self.ep_collision_rate,\n",
    "                   \"Ep_Wall_Collision_Rate\" : self.ep_wall_collision_rate, \"Ep_Obstacle_Collision_Rate\" : self.ep_obstacle_collision_rate,\n",
    "                   \"Ep_Human_Collision_Rate\" : self.ep_human_collision_rate, \"Ep_Path_Length\" : self.current_step,\n",
    "                   \"Ep_Stalled_Time\" : self.stalled_time, \"Ep_Discomfort\" : self.ep_discomfort,\n",
    "                   \"Ep_Avg_Human_Distance\" : self.ep_human_distance, \"Ep_Closest_Human_Distance\" : self.closest_human_distance,\n",
    "                   \"Ep_Close_Calls\" : self.close_call, \"Ep_Group_Inhibition\" : self.ep_group_inhibition_rate\n",
    "                   \n",
    "                   \"Avg_Collision_Rate\" : self.Avg_Collision_Rate, \"Avg_Min_Time_To_Collision\" : self.Avg_Min_Time_To_Collision,\n",
    "                   \"Avg_Wall_Collision_Rate\" : self.Avg_Wall_Collision_Rate, \"Avg_Obstacle_Collision_Rate\" : self.Avg_Obstacle_Collision_Rate,\n",
    "                   \"Avg_Human_Collision_Rate\" : self.Avg_Human_Collision_Rate, \"Avg_Path_Length\" : self.Avg_Path_Length,\n",
    "                   \"Avg_Stalled_Time\" : self.Avg_Stalled_Time, \"Avg_Discomfort\" : self.Avg_Discomfort,\n",
    "                   \"Avg_Human_Distance\" : self.Avg_Human_Distance, \"Avg_Closest_Human_Distance\" : self.Avg_Closest_Human_Distance,\n",
    "                   \"Avg_Group_Inhibition_Rate\" : self.Avg_Group_Inhibition_Rate\n",
    "                   })\n",
    "        '''\n",
    "        \n",
    "        print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "        print(f\"RUN DETAILS!!! \\n\")\n",
    "        print(f\"Ep_Total_Reward : {self.total_reward} \\n\"), \n",
    "        print(f\"Ep_Collision_Count : {self.collision_count}\")\n",
    "        print(f\"Ep_Wall_Collision_Count : {self.wall_collision_count}\")\n",
    "        print(f\"Ep_Obstacle_Collision_Count : {self.obstacle_collision_count}\")\n",
    "        print(f\"Ep_Human_Collision_Count : {self.human_collision_count}\")\n",
    "        print(f\"Ep_Min_Time_To_Collision : {self.min_time_to_collision}\")\n",
    "        print(f\"Ep_Stalled_Time : {self.stalled_time}\")\n",
    "        print(f\"Ep_Avg_Human_Distance : {self.Avg_Human_Distance}\")\n",
    "        \n",
    "        print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "        print(\"Creating the new Episode\")\n",
    "        \n",
    "    # You can optionally handle the 'seed' or other kwargs if needed\n",
    "        if 'seed' in kwargs:\n",
    "            np.random.seed(kwargs['seed'])\n",
    "\n",
    "        # self.agent_pos = np.random.uniform(0, self.grid_size, size=2).round(2)\n",
    "        # self.goal_pos = np.random.uniform(0, self.grid_size, size=2).round(2)\n",
    "        # self.num_static_obstacles = np.random.randint(1,10)\n",
    "        # self.num_dynamic_obstacles = np.random.randint(1,21)\n",
    "\n",
    "        self.agent_pos = np.array(np.random.randint(0, self.grid_size, size=2)).astype('float')\n",
    "        self.goal_pos = np.array(np.random.randint(0, self.grid_size, size=2)).astype('float')\n",
    "        self.num_static_obstacles = 30\n",
    "        self.num_dynamic_obstacles = 20\n",
    "        self.dynamic_obstacles = self._init_dynamic_obstacles()\n",
    "        self.static_obstacles = self._init_static_obstacles()\n",
    "        self.safe = True\n",
    "        self.done = False\n",
    "        \n",
    "        # Evaluation Metrics\n",
    "\n",
    "        self.Avg_Success_Rate = 0\n",
    "        self.ep_collision_rate, self.collision_count = 0, 0\n",
    "        self.min_time_to_collision = 0\n",
    "        self.ep_wall_collision_rate, self.wall_collision_count = 0, 0\n",
    "        self.ep_obstacle_collision_rate, self.obstacle_collision_count = 0, 0\n",
    "        self.ep_human_collision_rate, self.human_collision_count = 0, 0\n",
    "        \n",
    "        self.Avg_Timeout = 0\n",
    "        self.Avg_Path_Length = 0\n",
    "        self.stalled_time = 0\n",
    "\n",
    "        self.ep_group_inhibition_rate, self.group_inhibition = 0, 0\n",
    "        self.ep_discomfort = 0\n",
    "        self.ep_human_distance, self.human_distance = 0, 0\n",
    "        self.closest_human_distance = 0\n",
    "        self.Min_Closest_Human_Distance = 0\n",
    "                \n",
    "        self.close_call, self.discomfort, self.current_step = 0, 0, 1\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        print(\"*******************************************************************\\n\")\n",
    "        print(\"Initialized the environment with the following\")\n",
    "        print(\"Agent's Initial Position :\", self.agent_pos)\n",
    "        print(\"Goal Position :\", self.goal_pos)\n",
    "        print(\"Number of Static Obstacles :\", self.num_static_obstacles)\n",
    "        print(\"Static Obstacle Positions :\", self. static_obstacles)\n",
    "        print(\"Number of Dynamic Obstacles :\", self.num_dynamic_obstacles)\n",
    "        print(\"Dynamic Obstacle theta & dist :\", self. dynamic_obstacles, \"\\n\")\n",
    "        print(\"*******************************************************************\")\n",
    "        \n",
    "        return self._get_obs(), {}  # Ensure this returns a single dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import numpy as np\n",
    "\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, render_freq=1, save_freq=1, save_path=None, eval_env=None, verbose=1):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.render_freq = render_freq\n",
    "        self.save_freq = save_freq\n",
    "        self.save_path = save_path\n",
    "        self.eval_env = eval_env\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.best_model_path = os.path.join(save_path, 'Best_Models', \"best_model.zip\")\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \n",
    "        if self.n_calls % self.save_freq == 0:\n",
    "            save_file = os.path.join(self.save_path, f\"model_step_{self.n_calls}.zip\")\n",
    "            self.model.save(save_file)\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Model saved at step {self.n_calls} to {save_file}\")\n",
    "            \n",
    "            # Evaluate the model and save the best one\n",
    "            if self.eval_env is not None:\n",
    "                mean_reward = self.evaluate_model()\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    self.model.save(self.best_model_path)\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"New best model with mean reward {mean_reward} saved to {self.best_model_path}\")\n",
    "        \n",
    "        if self.n_calls % self.render_freq == 0:\n",
    "            self.env.render()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def evaluate_model(self, n_eval_episodes=5):\n",
    "        print(\"Calling to Evaluate!\")\n",
    "        all_episode_rewards = []\n",
    "        for _ in range(n_eval_episodes):\n",
    "            episode_rewards = []\n",
    "            obs = self.eval_env.reset()\n",
    "            done, state = False, None\n",
    "            while not done:\n",
    "                action, = self.model.predict(obs, deterministic=True)\n",
    "                obs, reward, done, info, _ = self.eval_env.step(action)\n",
    "                episode_rewards.append(reward)\n",
    "            all_episode_rewards.append(sum(episode_rewards))\n",
    "        mean_reward = np.mean(all_episode_rewards)\n",
    "        return mean_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINTING & LOGGING!!!\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "RUN DETAILS!!! \n",
      "\n",
      "Ep_Total_Reward : 0 \n",
      "\n",
      "Ep_Collision_Count : 0\n",
      "Ep_Wall_Collision_Count : 0\n",
      "Ep_Obstacle_Collision_Count : 0\n",
      "Ep_Human_Collision_Count : 0\n",
      "Ep_Min_Time_To_Collision : 0\n",
      "Ep_Stalled_Time : 0\n",
      "Ep_Avg_Human_Distance : 0.0\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Creating the new Episode\n",
      "*******************************************************************\n",
      "\n",
      "Initialized the environment with the following\n",
      "Agent's Initial Position : [8. 0.]\n",
      "Goal Position : [6. 8.]\n",
      "Number of Static Obstacles : 30\n",
      "Static Obstacle Positions : [array([4., 7.]), array([2., 6.]), array([6., 8.]), array([1., 2.]), array([6., 5.]), array([9., 1.]), array([4., 7.]), array([2., 8.]), array([3., 7.]), array([3., 9.]), array([1., 2.]), array([2., 3.]), array([3., 1.]), array([6., 7.]), array([8., 1.]), array([5., 3.]), array([9., 3.]), array([7., 6.]), array([7., 8.]), array([4., 7.]), array([9., 8.]), array([4., 9.]), array([5., 8.]), array([5., 3.]), array([4., 6.]), array([5., 1.]), array([7., 1.]), array([4., 6.]), array([4., 8.]), array([2., 9.])]\n",
      "Number of Dynamic Obstacles : 20\n",
      "Dynamic Obstacle theta & dist : [{'start': array([1., 0.]), 'end': array([8., 6.]), 'current': array([1., 0.]), 'angle': 0.0, 'distance': 7.0, 'path': []}, {'start': array([8., 5.]), 'end': array([4., 2.]), 'current': array([8., 5.]), 'angle': -90.0, 'distance': 5.0, 'path': []}, {'start': array([4., 0.]), 'end': array([0., 8.]), 'current': array([4., 0.]), 'angle': 0.0, 'distance': 4.0, 'path': []}, {'start': array([1., 3.]), 'end': array([9., 2.]), 'current': array([1., 3.]), 'angle': -23.19859051364819, 'distance': 7.615773105863909, 'path': []}, {'start': array([6., 1.]), 'end': array([1., 9.]), 'current': array([6., 1.]), 'angle': -26.56505117707799, 'distance': 2.23606797749979, 'path': []}, {'start': array([4., 3.]), 'end': array([8., 2.]), 'current': array([4., 3.]), 'angle': -36.86989764584402, 'distance': 5.0, 'path': []}, {'start': array([4., 2.]), 'end': array([6., 1.]), 'current': array([4., 2.]), 'angle': -26.56505117707799, 'distance': 4.47213595499958, 'path': []}, {'start': array([4., 5.]), 'end': array([1., 5.]), 'current': array([4., 5.]), 'angle': -51.34019174590991, 'distance': 6.4031242374328485, 'path': []}, {'start': array([3., 5.]), 'end': array([2., 3.]), 'current': array([3., 5.]), 'angle': -45.0, 'distance': 7.0710678118654755, 'path': []}, {'start': array([4., 5.]), 'end': array([2., 9.]), 'current': array([4., 5.]), 'angle': -51.34019174590991, 'distance': 6.4031242374328485, 'path': []}, {'start': array([8., 2.]), 'end': array([8., 4.]), 'current': array([8., 2.]), 'angle': -90.0, 'distance': 2.0, 'path': []}, {'start': array([3., 0.]), 'end': array([8., 7.]), 'current': array([3., 0.]), 'angle': 0.0, 'distance': 5.0, 'path': []}, {'start': array([2., 9.]), 'end': array([3., 4.]), 'current': array([2., 9.]), 'angle': -56.309932474020215, 'distance': 10.816653826391969, 'path': []}, {'start': array([4., 4.]), 'end': array([0., 7.]), 'current': array([4., 4.]), 'angle': -45.0, 'distance': 5.656854249492381, 'path': []}, {'start': array([7., 2.]), 'end': array([5., 4.]), 'current': array([7., 2.]), 'angle': -63.43494882292201, 'distance': 2.23606797749979, 'path': []}, {'start': array([8., 7.]), 'end': array([1., 3.]), 'current': array([8., 7.]), 'angle': -90.0, 'distance': 7.0, 'path': []}, {'start': array([6., 7.]), 'end': array([8., 0.]), 'current': array([6., 7.]), 'angle': -74.05460409907715, 'distance': 7.280109889280518, 'path': []}, {'start': array([8., 5.]), 'end': array([3., 6.]), 'current': array([8., 5.]), 'angle': -90.0, 'distance': 5.0, 'path': []}, {'start': array([9., 0.]), 'end': array([2., 4.]), 'current': array([9., 0.]), 'angle': 180.0, 'distance': 1.0, 'path': []}, {'start': array([9., 6.]), 'end': array([3., 9.]), 'current': array([9., 6.]), 'angle': -99.46232220802563, 'distance': 6.082762530298219, 'path': []}] \n",
      "\n",
      "*******************************************************************\n"
     ]
    }
   ],
   "source": [
    "env = DGSN_Env()\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Train','Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(9)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('agent': Box(0.0, 10.0, (2,), float32), 'dyn_obs': Box(0.0, 10.0, (20, 2), float64), 'sta_obs': Box(0.0, 10.0, (30, 2), float64))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('agent', array([2.2161038, 4.137377 ], dtype=float32)),\n",
       "             ('dyn_obs',\n",
       "              array([[6.42336539, 1.61994331],\n",
       "                     [2.26208877, 8.53979484],\n",
       "                     [7.17519979, 2.495168  ],\n",
       "                     [4.95615407, 2.13581935],\n",
       "                     [2.57404639, 7.10997218],\n",
       "                     [9.90220049, 2.87810875],\n",
       "                     [4.33517955, 4.81161951],\n",
       "                     [3.69035576, 9.96522584],\n",
       "                     [7.03023117, 2.20171672],\n",
       "                     [0.35866893, 6.09241907],\n",
       "                     [8.95709963, 8.2800882 ],\n",
       "                     [4.3548733 , 2.34686035],\n",
       "                     [1.32219184, 5.02541561],\n",
       "                     [4.23708498, 0.93705302],\n",
       "                     [8.50673416, 5.01939037],\n",
       "                     [4.90990237, 2.06514756],\n",
       "                     [6.66593979, 7.7686266 ],\n",
       "                     [9.74039922, 7.94862151],\n",
       "                     [3.37393024, 4.53175184],\n",
       "                     [7.81798582, 4.07710609]])),\n",
       "             ('sta_obs',\n",
       "              array([[3.13396375e+00, 8.60298353e+00],\n",
       "                     [9.51296609e+00, 9.73362488e+00],\n",
       "                     [5.72613540e+00, 9.71037426e+00],\n",
       "                     [7.89368303e+00, 2.40865883e+00],\n",
       "                     [7.46262271e-02, 6.24273911e+00],\n",
       "                     [6.92971498e+00, 6.02389013e+00],\n",
       "                     [8.22929605e+00, 3.54114931e+00],\n",
       "                     [7.81827196e-01, 5.60162158e-01],\n",
       "                     [7.18298472e+00, 4.69724831e+00],\n",
       "                     [1.71889507e+00, 7.11916532e+00],\n",
       "                     [1.68203114e+00, 9.11415820e+00],\n",
       "                     [1.24811890e-01, 4.85584108e+00],\n",
       "                     [6.68205678e+00, 5.85278332e+00],\n",
       "                     [7.28667541e+00, 1.71079155e+00],\n",
       "                     [6.50079919e+00, 1.70763651e+00],\n",
       "                     [9.88611798e+00, 9.01944502e+00],\n",
       "                     [4.35245870e+00, 4.73211206e-01],\n",
       "                     [1.12202281e+00, 2.20694437e-03],\n",
       "                     [2.25093948e+00, 8.79772812e+00],\n",
       "                     [4.83426625e+00, 7.96695279e+00],\n",
       "                     [8.44131881e+00, 3.68641489e-01],\n",
       "                     [1.27511966e+00, 8.43814925e+00],\n",
       "                     [8.11804418e+00, 2.45739769e+00],\n",
       "                     [6.00781040e+00, 5.48289998e+00],\n",
       "                     [5.98096445e+00, 9.31579080e-01],\n",
       "                     [3.08078926e+00, 9.44031485e+00],\n",
       "                     [6.99260412e+00, 1.98877289e+00],\n",
       "                     [7.83177544e+00, 6.47724453e+00],\n",
       "                     [4.42136219e+00, 2.56122215e+00],\n",
       "                     [4.74311262e+00, 1.70078021e+00]]))])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Train', 'Saved_Models', 'PPO_2')\n",
    "\n",
    "# Initialize the combined callback with rendering, saving, and evaluation functionality\n",
    "combined_callback = CustomCallback(env=env, render_freq=1, save_freq=1, save_path=save_path, eval_env=env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\"MultiInputPolicy\", env, verbose=1, tensorboard_log=log_path, device='cuda', n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINTING & LOGGING!!!\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "RUN DETAILS!!! \n",
      "\n",
      "Ep_Total_Reward : 0 \n",
      "\n",
      "Ep_Collision_Count : 0\n",
      "Ep_Wall_Collision_Count : 0\n",
      "Ep_Obstacle_Collision_Count : 0\n",
      "Ep_Human_Collision_Count : 0\n",
      "Ep_Min_Time_To_Collision : 0\n",
      "Ep_Stalled_Time : 0\n",
      "Ep_Avg_Human_Distance : 31.487465316154765\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Creating the new Episode\n",
      "*******************************************************************\n",
      "\n",
      "Initialized the environment with the following\n",
      "Agent's Initial Position : [8. 5.]\n",
      "Goal Position : [5. 5.]\n",
      "Number of Static Obstacles : 30\n",
      "Static Obstacle Positions : [array([4., 4.]), array([8., 5.]), array([5., 4.]), array([4., 8.]), array([6., 7.]), array([1., 1.]), array([5., 7.]), array([3., 5.]), array([6., 4.]), array([9., 1.]), array([6., 8.]), array([8., 4.]), array([7., 6.]), array([3., 7.]), array([2., 8.]), array([2., 1.]), array([8., 3.]), array([3., 1.]), array([9., 3.]), array([8., 1.]), array([2., 2.]), array([2., 3.]), array([1., 5.]), array([2., 8.]), array([7., 2.]), array([8., 3.]), array([6., 1.]), array([6., 8.]), array([2., 3.]), array([4., 1.])]\n",
      "Number of Dynamic Obstacles : 20\n",
      "Dynamic Obstacle theta & dist : [{'start': array([2., 5.]), 'end': array([3., 0.]), 'current': array([2., 5.]), 'angle': 0.0, 'distance': 6.0, 'path': []}, {'start': array([3., 6.]), 'end': array([1., 0.]), 'current': array([3., 6.]), 'angle': -11.309932474020213, 'distance': 5.0990195135927845, 'path': []}, {'start': array([0., 1.]), 'end': array([0., 9.]), 'current': array([0., 1.]), 'angle': 26.56505117707799, 'distance': 8.94427190999916, 'path': []}, {'start': array([2., 7.]), 'end': array([8., 0.]), 'current': array([2., 7.]), 'angle': -18.43494882292201, 'distance': 6.324555320336759, 'path': []}, {'start': array([4., 0.]), 'end': array([8., 2.]), 'current': array([4., 0.]), 'angle': 51.34019174590991, 'distance': 6.4031242374328485, 'path': []}, {'start': array([6., 6.]), 'end': array([8., 9.]), 'current': array([6., 6.]), 'angle': -26.56505117707799, 'distance': 2.23606797749979, 'path': []}, {'start': array([8., 4.]), 'end': array([7., 5.]), 'current': array([8., 4.]), 'angle': 90.0, 'distance': 1.0, 'path': []}, {'start': array([9., 0.]), 'end': array([0., 4.]), 'current': array([9., 0.]), 'angle': 101.30993247402021, 'distance': 5.0990195135927845, 'path': []}, {'start': array([7., 8.]), 'end': array([9., 8.]), 'current': array([7., 8.]), 'angle': -71.56505117707799, 'distance': 3.1622776601683795, 'path': []}, {'start': array([8., 0.]), 'end': array([7., 4.]), 'current': array([8., 0.]), 'angle': 90.0, 'distance': 5.0, 'path': []}, {'start': array([4., 9.]), 'end': array([0., 0.]), 'current': array([4., 9.]), 'angle': -45.0, 'distance': 5.656854249492381, 'path': [array([4., 9.]), array([4. , 8.9]), array([4. , 8.8]), array([4. , 8.7]), array([4. , 8.6]), array([4. , 8.5]), array([4. , 8.4]), array([4. , 8.3]), array([4. , 8.2]), array([4. , 8.1]), array([4., 8.]), array([4. , 7.9]), array([4. , 7.8]), array([4. , 7.7]), array([4. , 7.6]), array([4. , 7.5]), array([4. , 7.4]), array([4. , 7.3]), array([4. , 7.2]), array([4. , 7.1]), array([4., 7.]), array([4. , 6.9]), array([4. , 6.8]), array([4. , 6.7]), array([4. , 6.6]), array([4. , 6.5]), array([4. , 6.4]), array([4. , 6.3]), array([4. , 6.2]), array([4. , 6.1]), array([4., 6.]), array([4. , 5.9]), array([4. , 5.8]), array([4. , 5.7]), array([4. , 5.6]), array([4. , 5.5]), array([4. , 5.4]), array([4. , 5.3]), array([4. , 5.2]), array([4. , 5.1]), array([4., 5.]), array([4. , 4.9]), array([4. , 4.8]), array([4. , 4.7]), array([4. , 4.6]), array([4. , 4.5]), array([4. , 4.4]), array([4. , 4.3]), array([4. , 4.2]), array([4. , 4.1]), array([4., 4.]), array([3.9, 4. ]), array([3.9, 3.9]), array([3.8, 3.9]), array([3.8, 3.8]), array([3.7, 3.8]), array([3.7, 3.7]), array([3.7, 3.6]), array([3.6, 3.6]), array([3.5, 3.6]), array([3.5, 3.5]), array([3.4, 3.5]), array([3.4, 3.4]), array([3.3, 3.4]), array([3.3, 3.3]), array([3.2, 3.3]), array([3.2, 3.2]), array([3.1, 3.2]), array([3.1, 3.1]), array([3. , 3.1]), array([3., 3.]), array([3. , 2.9]), array([2.9, 2.9]), array([2.8, 2.9]), array([2.8, 2.8]), array([2.8, 2.7]), array([2.7, 2.7]), array([2.7, 2.6]), array([2.6, 2.6]), array([2.6, 2.5]), array([2.5, 2.5]), array([2.4, 2.5]), array([2.4, 2.4]), array([2.4, 2.3]), array([2.3, 2.3]), array([2.2, 2.3]), array([2.2, 2.2]), array([2.1, 2.2]), array([2.1, 2.1]), array([2.1, 2. ]), array([2., 2.]), array([1.9, 2. ]), array([1.9, 1.9]), array([1.8, 1.9]), array([1.8, 1.8]), array([1.7, 1.8]), array([1.7, 1.7]), array([1.7, 1.6]), array([1.6, 1.6]), array([1.6, 1.5]), array([1.5, 1.5]), array([1.5, 1.4]), array([1.4, 1.4]), array([1.4, 1.3]), array([1.3, 1.3]), array([1.2, 1.3]), array([1.2, 1.2]), array([1.2, 1.1]), array([1.1, 1.1]), array([1.1, 1. ]), array([1., 1.]), array([1. , 0.9]), array([0.9, 0.9]), array([0.9, 0.8]), array([0.8, 0.8]), array([0.8, 0.7]), array([0.7, 0.7]), array([0.7, 0.6]), array([0.6, 0.6]), array([0.6, 0.5]), array([0.5, 0.5]), array([0.5, 0.4]), array([0.4, 0.4]), array([0.4, 0.3]), array([0.3, 0.3]), array([0.3, 0.2]), array([0.2, 0.2]), array([0.2, 0.1]), array([0.1, 0.1]), array([1.0000000e-01, 1.5237811e-14]), array([0.0000000e+00, 1.5237811e-14]), array([0., 0.])]}, {'start': array([2., 1.]), 'end': array([7., 9.]), 'current': array([2., 1.]), 'angle': 33.690067525979785, 'distance': 7.211102550927978, 'path': []}, {'start': array([8., 5.]), 'end': array([8., 8.]), 'current': array([8., 5.]), 'angle': 0.0, 'distance': 0.0, 'path': []}, {'start': array([0., 8.]), 'end': array([9., 7.]), 'current': array([0., 8.]), 'angle': -20.556045219583467, 'distance': 8.54400374531753, 'path': []}, {'start': array([5., 4.]), 'end': array([2., 4.]), 'current': array([5., 4.]), 'angle': 18.43494882292201, 'distance': 3.1622776601683795, 'path': [array([5., 4.]), array([4.9, 4. ]), array([4.8, 4. ]), array([4.7, 4. ]), array([4.6, 4. ]), array([4.5, 4. ]), array([4.4, 4. ]), array([4.3, 4. ]), array([4.2, 4. ]), array([4.1, 4. ]), array([4., 4.]), array([3.9, 4. ]), array([3.8, 4. ]), array([3.7, 4. ]), array([3.6, 4. ]), array([3.5, 4. ]), array([3.4, 4. ]), array([3.3, 4. ]), array([3.2, 4. ]), array([3.1, 4. ]), array([3., 4.]), array([2.9, 4. ]), array([2.8, 4. ]), array([2.7, 4. ]), array([2.6, 4. ]), array([2.5, 4. ]), array([2.4, 4. ]), array([2.3, 4. ]), array([2.2, 4. ]), array([2.1, 4. ]), array([2., 4.]), array([1.9, 4. ]), array([1.8, 4. ]), array([1.7, 4. ]), array([1.6, 4. ]), array([1.5, 4. ]), array([1.4, 4. ]), array([1.3, 4. ]), array([1.2, 4. ]), array([1.1, 4. ]), array([1., 4.]), array([0.9, 4. ]), array([0.8, 4. ]), array([0.7, 4. ]), array([0.6, 4. ]), array([0.5, 4. ]), array([0.4, 4. ]), array([0.3, 4. ]), array([0.2, 4. ]), array([0.1, 4. ]), array([1.0269563e-15, 4.0000000e+00]), array([0., 4.]), array([0.1, 4. ]), array([0.2, 4. ]), array([0.3, 4. ]), array([0.4, 4. ]), array([0.5, 4. ]), array([0.6, 4. ]), array([0.7, 4. ]), array([0.8, 4. ]), array([0.9, 4. ]), array([1., 4.]), array([1.1, 4. ]), array([1.2, 4. ]), array([1.3, 4. ]), array([1.4, 4. ]), array([1.5, 4. ]), array([1.6, 4. ]), array([1.7, 4. ]), array([1.8, 4. ]), array([1.9, 4. ]), array([2., 4.]), array([2.1, 4. ]), array([2.2, 4. ]), array([2.3, 4. ]), array([2.4, 4. ]), array([2.5, 4. ]), array([2.6, 4. ]), array([2.7, 4. ]), array([2.8, 4. ]), array([2.9, 4. ]), array([3., 4.]), array([3.1, 4. ]), array([3.2, 4. ]), array([3.3, 4. ]), array([3.4, 4. ]), array([3.5, 4. ]), array([3.6, 4. ]), array([3.7, 4. ]), array([3.8, 4. ]), array([3.9, 4. ]), array([4., 4.]), array([3.9, 4. ]), array([3.8, 4. ]), array([3.7, 4. ]), array([3.6, 4. ]), array([3.5, 4. ]), array([3.4, 4. ]), array([3.3, 4. ]), array([3.2, 4. ]), array([3.1, 4. ]), array([3., 4.]), array([2.9, 4. ]), array([2.8, 4. ]), array([2.7, 4. ]), array([2.6, 4. ]), array([2.5, 4. ]), array([2.4, 4. ]), array([2.3, 4. ]), array([2.2, 4. ]), array([2.1, 4. ]), array([2., 4.])]}, {'start': array([7., 9.]), 'end': array([8., 4.]), 'current': array([7., 9.]), 'angle': -75.96375653207353, 'distance': 4.123105625617661, 'path': []}, {'start': array([1., 9.]), 'end': array([0., 4.]), 'current': array([1., 9.]), 'angle': -29.74488129694222, 'distance': 8.06225774829855, 'path': []}, {'start': array([4., 7.]), 'end': array([3., 6.]), 'current': array([4., 7.]), 'angle': -26.56505117707799, 'distance': 4.47213595499958, 'path': []}, {'start': array([6., 3.]), 'end': array([5., 7.]), 'current': array([6., 3.]), 'angle': 45.0, 'distance': 2.8284271247461903, 'path': []}, {'start': array([0., 1.]), 'end': array([6., 2.]), 'current': array([0., 1.]), 'angle': 26.56505117707799, 'distance': 8.94427190999916, 'path': []}] \n",
      "\n",
      "*******************************************************************\n",
      "Logging to Train\\Logs\\PPO_37\n",
      "Agent's Current Position [8. 5.]\n",
      "Goal Position : [5. 5.]\n",
      "Action Taken : 4 - Right-Up\n",
      "Agent's New Position [8.2 5.2]\n",
      "Steps taken : 2\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \n",
      "\n",
      "Distance to the Goal : 3.206243908376279\n",
      "Del_Distance : 0.20624390837627882\n",
      "Reward from Del_Dist : -0.41248781675255763\n",
      "******************************\n",
      "Obstacle #0 Distance : 6.203224967708328\n",
      "******************************\n",
      "Obstacle #1 Distance : 5.261178575186362\n",
      "******************************\n",
      "Obstacle #2 Distance : 9.213034245024817\n",
      "******************************\n",
      "Obstacle #3 Distance : 6.456004956627589\n",
      "******************************\n",
      "Obstacle #4 Distance : 6.684309986827361\n",
      "******************************\n",
      "Obstacle #5 Distance : 2.3409399821439245\n",
      "Penalty Obtained : 4.271788288583806\n",
      "******************************\n",
      "Obstacle #6 Distance : 1.216552506059644\n",
      "Penalty Obtained : 8.219949365267864\n",
      "******************************\n",
      "Obstacle #7 Distance : 5.261178575186363\n",
      "******************************\n",
      "Obstacle #8 Distance : 3.046309242345563\n",
      "******************************\n",
      "Obstacle #9 Distance : 5.203844732503075\n",
      "******************************\n",
      "Obstacle #10 Distance : 5.663920903402517\n",
      "******************************\n",
      "Obstacle #11 Distance : 7.488658090739621\n",
      "******************************\n",
      "Obstacle #12 Distance : 0.2828427124746186\n",
      "Penalty Obtained : 35.35533905932743\n",
      "******************************\n",
      "Obstacle #13 Distance : 8.664871608973788\n",
      "******************************\n",
      "Obstacle #14 Distance : 3.417601498127012\n",
      "******************************\n",
      "Obstacle #15 Distance : 3.9849717690342548\n",
      "******************************\n",
      "Obstacle #16 Distance : 8.141252974819047\n",
      "******************************\n",
      "Obstacle #17 Distance : 4.569463863518344\n",
      "******************************\n",
      "Obstacle #18 Distance : 3.1112698372208087\n",
      "******************************\n",
      "Obstacle #19 Distance : 9.213034245024817\n",
      "Reward Post Dynamic Manouevres : -48.25956452993165\n",
      "******************************\n",
      "#####################################\n",
      "Reward Obtained : -49.25956452993165\n",
      "#####################################\n",
      "Model saved at step 1 to Train\\Saved_Models\\PPO_2\\model_step_1.zip\n",
      "Calling to Evaluate!\n",
      "PRINTING & LOGGING!!!\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "RUN DETAILS!!! \n",
      "\n",
      "Ep_Total_Reward : -49.25956452993165 \n",
      "\n",
      "Ep_Collision_Count : 0\n",
      "Ep_Wall_Collision_Count : 0\n",
      "Ep_Obstacle_Collision_Count : 0\n",
      "Ep_Human_Collision_Count : 0\n",
      "Ep_Min_Time_To_Collision : 1\n",
      "Ep_Stalled_Time : 0\n",
      "Ep_Avg_Human_Distance : 33.84577279619023\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Creating the new Episode\n",
      "*******************************************************************\n",
      "\n",
      "Initialized the environment with the following\n",
      "Agent's Initial Position : [8. 2.]\n",
      "Goal Position : [0. 6.]\n",
      "Number of Static Obstacles : 30\n",
      "Static Obstacle Positions : [array([1., 9.]), array([9., 3.]), array([5., 7.]), array([7., 5.]), array([3., 6.]), array([1., 3.]), array([1., 5.]), array([6., 4.]), array([9., 1.]), array([1., 2.]), array([1., 5.]), array([8., 6.]), array([4., 9.]), array([3., 8.]), array([4., 8.]), array([4., 9.]), array([3., 4.]), array([3., 6.]), array([7., 6.]), array([5., 1.]), array([8., 8.]), array([2., 8.]), array([5., 5.]), array([7., 7.]), array([2., 3.]), array([8., 7.]), array([7., 7.]), array([8., 3.]), array([9., 2.]), array([9., 6.])]\n",
      "Number of Dynamic Obstacles : 20\n",
      "Dynamic Obstacle theta & dist : [{'start': array([6., 6.]), 'end': array([6., 8.]), 'current': array([6., 6.]), 'angle': -63.43494882292201, 'distance': 4.47213595499958, 'path': []}, {'start': array([7., 8.]), 'end': array([4., 2.]), 'current': array([7., 8.]), 'angle': -80.53767779197439, 'distance': 6.082762530298219, 'path': []}, {'start': array([7., 3.]), 'end': array([9., 9.]), 'current': array([7., 3.]), 'angle': -45.0, 'distance': 1.4142135623730951, 'path': []}, {'start': array([7., 7.]), 'end': array([5., 8.]), 'current': array([7., 7.]), 'angle': -78.69006752597979, 'distance': 5.0990195135927845, 'path': []}, {'start': array([6., 1.]), 'end': array([3., 8.]), 'current': array([6., 1.]), 'angle': 26.56505117707799, 'distance': 2.23606797749979, 'path': []}, {'start': array([0., 8.]), 'end': array([6., 9.]), 'current': array([0., 8.]), 'angle': -36.86989764584402, 'distance': 10.0, 'path': []}, {'start': array([6., 3.]), 'end': array([9., 8.]), 'current': array([6., 3.]), 'angle': -26.56505117707799, 'distance': 2.23606797749979, 'path': []}, {'start': array([5., 9.]), 'end': array([8., 8.]), 'current': array([5., 9.]), 'angle': -66.80140948635182, 'distance': 7.615773105863909, 'path': []}, {'start': array([5., 7.]), 'end': array([4., 6.]), 'current': array([5., 7.]), 'angle': -59.03624346792648, 'distance': 5.830951894845301, 'path': []}, {'start': array([9., 9.]), 'end': array([2., 5.]), 'current': array([9., 9.]), 'angle': -98.13010235415598, 'distance': 7.0710678118654755, 'path': []}, {'start': array([5., 4.]), 'end': array([8., 5.]), 'current': array([5., 4.]), 'angle': -33.690067525979785, 'distance': 3.605551275463989, 'path': []}, {'start': array([0., 5.]), 'end': array([4., 1.]), 'current': array([0., 5.]), 'angle': -20.556045219583467, 'distance': 8.54400374531753, 'path': []}, {'start': array([1., 8.]), 'end': array([9., 5.]), 'current': array([1., 8.]), 'angle': -40.60129464500447, 'distance': 9.219544457292887, 'path': []}, {'start': array([9., 8.]), 'end': array([5., 3.]), 'current': array([9., 8.]), 'angle': -99.46232220802563, 'distance': 6.082762530298219, 'path': []}, {'start': array([7., 8.]), 'end': array([9., 0.]), 'current': array([7., 8.]), 'angle': -80.53767779197439, 'distance': 6.082762530298219, 'path': []}, {'start': array([6., 2.]), 'end': array([7., 6.]), 'current': array([6., 2.]), 'angle': 0.0, 'distance': 2.0, 'path': []}, {'start': array([9., 4.]), 'end': array([1., 4.]), 'current': array([9., 4.]), 'angle': -116.56505117707799, 'distance': 2.23606797749979, 'path': []}, {'start': array([4., 7.]), 'end': array([1., 8.]), 'current': array([4., 7.]), 'angle': -51.34019174590991, 'distance': 6.4031242374328485, 'path': []}, {'start': array([8., 7.]), 'end': array([5., 5.]), 'current': array([8., 7.]), 'angle': -90.0, 'distance': 5.0, 'path': []}, {'start': array([0., 7.]), 'end': array([4., 1.]), 'current': array([0., 7.]), 'angle': -32.005383208083494, 'distance': 9.433981132056603, 'path': []}] \n",
      "\n",
      "*******************************************************************\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\myRL\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\myRL\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\myRL\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:201\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[0;32m    200\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[1;32m--> 201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_info_buffer(infos, dones)\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\myRL\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[99], line 30\u001b[0m, in \u001b[0;36mCustomCallback._on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Evaluate the model and save the best one\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     mean_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mean_reward \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_mean_reward:\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_mean_reward \u001b[38;5;241m=\u001b[39m mean_reward\n",
      "Cell \u001b[1;32mIn[99], line 50\u001b[0m, in \u001b[0;36mCustomCallback.evaluate_model\u001b[1;34m(self, n_eval_episodes)\u001b[0m\n\u001b[0;32m     48\u001b[0m done, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 50\u001b[0m     action, \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     obs, reward, done, info, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     52\u001b[0m     episode_rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\myRL\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rahat\\miniconda3\\envs\\myRL\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:357\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m     )\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mValueError\u001b[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000000, callback=combined_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGiCAYAAADNzj2mAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAmdklEQVR4nO3df3AU933/8dchmVudg66AAncac5ZkNEdiO8YOgeHHtM2YmmYYTz1t43FKO8iS7X5TbMBqU6M24Lj+oUBSD4PtsYN7YDoNTTzTkqROHQ9DHDKeYLAhuPG0PovY5BSXg5Exd8DlhCvt948DmcudZIT2bj+3+3x4NGft3mnfO3u7++Kz+/lswLZtWwAAAAaZ5HYBAAAAv42AAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMM+6A8tOf/lS33nqrmpubFQgE9L3vfa9ovm3b2rBhg6LRqBoaGrR06VL19fU5VS8AAPCBcQeUs2fP6oYbbtBTTz1Vdv6mTZu0ZcsWPfPMM9q/f7+uvPJKLVu2TPl8fsLFAgAAfwhM5GGBgUBAu3bt0m233Sap0HrS3Nysv/7rv9bf/M3fSJIymYxmzpyp5557TnfccYcjRQMAAG+rd/KPvfvuu0qn01q6dOnItHA4rAULFmjfvn1lA8rg4KAGBwdHfh8eHtbJkyc1ffp0BQIBJ8sDAAAVYtu2Tp8+rebmZk2aNPFbXB0NKOl0WpI0c+bMoukzZ84cmffbent79dBDDzlZBgAAcEl/f7+uuuqqCf8dRwPK5ejp6VF3d/fI75lMRrFYTP39/WpsbHSxMgAAcKmy2axmzZqlKVOmOPL3HA0okUhEknT8+HFFo9GR6cePH9fcuXPLfiYYDCoYDJZMb2xsJKAAAFBjnLo9w9FxUFpbWxWJRLRnz56RadlsVvv379fChQudXBQAAPCwcbegnDlzRkeOHBn5/d1339Xhw4c1bdo0xWIxrV27Vo888oja29vV2tqq9evXq7m5eaSnDwAAwMcZd0B5/fXX9fnPf37k9wv3j6xcuVLPPfec/vZv/1Znz57VPffco1OnTmnJkiX60Y9+JMuynKsaAAB42oTGQamEbDarcDisTCbDPSgAANQIp8/fPIsHAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMU+92ATBTKiUNDJROb2qSYrHq1wOYgP3CHGwL7yOgoEQqJcXjUj5fOs+ypGSSAwD8h/3CHGwLf+ASD0oMDJTf8aXC9HL/agG8jv3CHGwLfyCgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKSjQ1Fe6EL8eyCvMBv2G/MAfbwh/oZowSsVihmx5jDAAfYb8wB9vCHwK2bdtuF3GxbDarcDisTCajxsZGt8sBAACXwOnzN5d4AACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjMJIsRpXryym9La380bysFkuRzohC7SG3ywIA+AABBWUd235MybuSUkCSLSkgpTalFE/EFe2Iul0eAMDjuMSDErm+XCGcDEsaUtFrsiup3JGcuwUCADyPgIIS6W3pQstJOQEpnUhXtR4AgP8QUFAifzRfuKxTjn1+PgAAFURAQQmrxRqzBcVqsapaDwDAfwgoKBHpjIzZghLpilS1HgCA/xBQUCLUHlI8ES98O+pU9BpPxBWaTVdjAEBl0c0YZUU7ogovCSuduGgclK4I4QQAUBUEFIwqNDuktt42t8sAAPgQl3gAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEcH0l2aGhIX/va1/Qv//IvSqfTam5uVkdHh7761a8qEBjtEbn+luvLKb3toiHlOyMKtbs7pHwqJQ0MlE5vapJiserXA/iJiccE03jlGOWV9agExwPKxo0b9fTTT2vHjh269tpr9frrr+vOO+9UOBzW6tWrnV5czTu2/ZiSdyWlgApPEA5IqU0pxRNxRTuirtSUSknxuJTPl86zLCmZZMcBKsXEY4JpvHKM8sp6VIrjl3h+9rOf6Y/+6I+0fPlytbS06E//9E91yy236MCBA04vqubl+nKFA9GwpCEVvSa7ksodyblS18BA+R1GKkwvl/YBTJypxwTTeOUY5ZX1qBTHA8qiRYu0Z88evf3225KkN954Q6+88oq+8IUvlH3/4OCgstls0Y9fpLelC/9KKicgpRPpqtYDwF0cE4CPOH6JZ926dcpms5ozZ47q6uo0NDSkRx99VCtWrCj7/t7eXj300ENOl1ET8kfzhSbccuzz8wH4BscE4COOt6A8//zz+va3v62dO3fq0KFD2rFjh775zW9qx44dZd/f09OjTCYz8tPf3+90ScayWqwx/7VktVhVrQeAuzgmAB9xvAXlK1/5itatW6c77rhDknT99dfrV7/6lXp7e7Vy5cqS9weDQQWDQafLqAmRzohSm1LlZ9pSpCtS3YIAuIpjAvARx1tQcrmcJk0q/rN1dXUaHh52elE1L9QeUjwRL2yFOhW9xhNxhWa7062wqalwB3k5llWYD8B5ph4TTOOVY5RX1qNSHG9BufXWW/Xoo48qFovp2muv1c9//nM9/vjj6uzsdHpRnhDtiCq8JKx04qIxD7oirh6IYrFC9zb65gPVZ+IxwTReOUZ5ZT0qJWDb9mi3ZF2W06dPa/369dq1a5dOnDih5uZmfelLX9KGDRs0efLkj/18NptVOBxWJpNRY2Ojk6UBGAODgwGYCKfP344HlIkioADVV25wMNlicDAAl8zp8zfP4gF8jsHBAJiIgAL4HIODATARAQXwOQYHA2AiAgrgcwwOBsBEBBTA5yKdkTFbUBgcDIAbCCiAzzE4GAATOT5QG4Daw+BgAExDQAEgSQrNDqmtt61if5+B4ACMBwEFQMWVGwgutSnFQHAARsU9KAAqioHgAFwOAgqAimIgOACXg4ACoKIYCA7A5SCgAKgoBoIDcDkIKAAqioHgAFwOAgqAimIgOACXg27GACqOgeAAjBcBBUBVVHogOADewiUeAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcRpIFDJdKSQMDpdObmqRYrPr1oPJyfTmlt130WIDOiELtPBYA/kJAAQyWSknxuJTPl86zLCmZJKR4zbHtx5S8KykFVHgKdEBKbUopnogr2hF1uzygarjEAxhsYKB8OJEK08u1rKB25fpyhXAyLGlIRa/JrqRyR3LuFghUEQEFAAyR3pYutJyUE5DSiXRV6wHcREABAEPkj+YLl3XKsc/PB3yCgAIAhrBarDFbUKwWq6r1AG4ioACAISKdkTFbUCJdkarWA7iJgAIYrKmp0FunHMsqzId3hNpDiifihSNznYpe44m4QrPpagz/oJsxYLBYrNCVmHFQ/CPaEVV4SVjpxEXjoHRFCCfwHQIKPM0LA17FYgQR3/ntyzyjXfYBPCxg27ZRX/1sNqtwOKxMJqPGxka3y0ENKzfglWwx4BWMxvcWtcrp8zf3oMCTGPAKtYjvLfARAgo8iQGvUIv43gIfIaDAkxjwCrWI7y3wEQIKPIkBr1CL+N4CHyGgwJMY8Aq1iO8t8BECCjyJAa9Qi/jeAh+hmzE8LXckx4BXqDl8b1GLnD5/E1AAAMCEMQ4KAADwPAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBSM6uTukzq08JD2Xb1PhxYe0sndJ90uCQDgE/VuFwAzvdX5ltLb0yO/D6YG9V+3/JciXRHN+ac5LlYGAPADWlBQ4uTuk0Xh5GLpRFon99CSAgCoLAIKShzdcHTs+V8dez4AABNFQEGJwf8dnNB8AAAmioCCEsHm4ITmAwAwUQQUlGj5h5ax5z8y9nwAACaKgIIS0/5gmiJdkbLzIl0RTbt5WpUrAgD4Dd2MUdacf5qjGV+aoaNfParB/x1UsDmolkdaCCcAgKogoGBU026eRiABALiiIgHlvffe0wMPPKAXX3xRuVxOs2fP1vbt2zVv3rxKLA4A4DOplDQwUDq9qUmKxapfz+XyynpUguMB5YMPPtDixYv1+c9/Xi+++KI++clPqq+vT1OnTnV6UQAAH0qlpHhcyudL51mWlEzWxsndK+tRKY4HlI0bN2rWrFnavn37yLTW1lanFwMA8KmBgfIndakwfWCgNk7sXlmPSnG8F88PfvADzZs3T1/84hc1Y8YM3XjjjXr22WdHff/g4KCy2WzRDwAA8DfHA8o777yjp59+Wu3t7XrppZf05S9/WatXr9aOHTvKvr+3t1fhcHjkZ9asWU6XBAAAaozjAWV4eFg33XSTHnvsMd1444265557dPfdd+uZZ54p+/6enh5lMpmRn/7+fqdLAgAANcbxgBKNRvXpT3+6aNqnPvUppVKpsu8PBoNqbGws+gEAAP7meEBZvHixkslk0bS3335bV199tdOLAgD4UFNToZdLOZZVmF8LvLIeleJ4L577779fixYt0mOPPabbb79dBw4c0NatW7V161anFwUA8KFYrNAFt9bHD/HKelRKwLZt2+k/+sILL6inp0d9fX1qbW1Vd3e37r777kv6bDabVTgcViaT4XIPAAA1wunzd0UCykQQUAAAqD1On795mjEAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDiOjyQL78j15ZTellb+aF5Wi6VIZ0Sh9pDbZQEAfICAgrKObT+m5F1JKSDJlhSQUptSiifiinZE3S4PAOBxXOJBiVxfrhBOhiUNqeg12ZVU7kjO3QIBAJ5HQEGJ9LZ0oeWknICUTqSrWg8AwH8IKCiRP5ovXNYpxz4/HwCACiKgoITVYo3ZgmK1WFWtBwDgPwQUlIh0RsZsQYl0RapaDwDAfwgoKBFqDymeiBe+HXUqeo0n4grNpqsxAKCy6GaMsqIdUYWXhJVOXDQOSleEcAIAqAoCCkYVmh1SW2+b22UAAHyISzwAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEYqA2elUpJAwOl05uapFis+vVcrmqtR64vp/S2i0YO7owo1M7IwQDcQUCBJ6VSUjwu5fOl8yxLSiZrI6RUaz2ObT+m5F3JwlOsbUkBKbUppXgirmhHdOILAIBx4hIPPGlgoPxJXSpML9ciYaJqrEeuL1cIJ8OShlT0muxKKnckN/GFAMA4EVAAn0tvSxdaTsoJSOlEuqr1AIBEQAF8L380X7isU459fj4AVBkBBfA5q8UaswXFarGqWg8ASAQUwPcinZExW1AiXZGq1gMAEgEFHtXUVOjlUo5lFebXgmqsR6g9pHgiXjga1KnoNZ6IKzSbrsYAqo9uxvCkWKzQBbfWx0Gp1npEO6IKLwkrnbhoHJSuCOEEgGsCtm2P1rjrimw2q3A4rEwmo8bGRrfLAQAAl8Dp8zeXeAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcRjqHgDwsXJ9OaW3XfQohM6IQu08CgGVQ0ABAIzp2PZjSt6VlAIqPPk6IKU2pRRPxBXtiLpdHjyKSzwAgFHl+nKFcDIsaUhFr8mupHJHcu4WCM8ioAAARpXeli60nJQTkNKJdFXrgX8QUAAAo8ofzRcu65Rjn58PVAABBQAwKqvFGrMFxWqxqloP/IOAAgAYVaQzMmYLSqQrUtV64B8EFADAqELtIcUT8cLZok5Fr/FEXKHZdDVGZdDNGAAwpmhHVOElYaUTF42D0hUhnKCiCCgAgI8Vmh1SW2+b22XAR7jEAwAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4HaHJZKSQMDpdObmqRYrPr1+Nm+fdI775ROb2uTFi6sfj2ofezf5vDK/s13anQVDyhf//rX1dPTozVr1mjz5s2VXpyrUikpHpfyZZ4+bllSMskXrlr27ZMWLRp9/s9+VlsHMbiP/dscXtm/+U6NraKXeF577TV961vf0mc+85lKLsYYAwPlv2hSYXq5lIzKKPcvq/HMB34b+7c5vLJ/850aW8UCypkzZ7RixQo9++yzmjp16qjvGxwcVDabLfoBAAD+VrGAsmrVKi1fvlxLly4d8329vb0Kh8MjP7NmzapUSQAAoEZUJKB85zvf0aFDh9Tb2/ux7+3p6VEmkxn56e/vr0RJAACghjh+k2x/f7/WrFmj3bt3y7Ksj31/MBhUMBh0ugwAAFDDHG9BOXjwoE6cOKGbbrpJ9fX1qq+v1969e7VlyxbV19draGjI6UUao6mpcOd1OZZVmI/qaGub2Hzgt7F/m8Mr+zffqbEFbNu2nfyDp0+f1q9+9auiaXfeeafmzJmjBx54QNddd92Yn89mswqHw8pkMmpsbHSytKqgT7s5vDJOAszB/m0Or+zfXvpOOX3+djyglPP7v//7mjt37iWNg1LrAQUAAD9y+vzNUPcAAMA4VRnq/ic/+Uk1FgMAADyCFhQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGCcercLgJm2bpUOHy6dPneudM891a7G31IpaWCgdHpTkxSLVb8eAKgGAgpKbN0q/eVfjv0eQkp1pFJSPC7l86XzLEtKJgkpALyJSzwoUa7lZDzz4ZyBgfLhRCpML9eyAgBeQEABAADGIaAAAADjEFAAAIBxCCgAAMA4BBSUmDt3YvPhnKamQm+dciyrMB8AvIhuxihxoQsx46C4LxYrdCVmHBQAfkNAQVmEEHPEYgQRAP5DQAFqQK4vp/S2tPJH87JaLEU6Iwq1h9wuCwAqhoACGO7Y9mNK3pWUApJsSQEptSmleCKuaEfU7fIAoCK4SRYwWK4vVwgnw5KGVPSa7EoqdyTnboEAUCEEFMBg6W3pQstJOQEpnUhXtR4AqBYCCmCw/NF84bJOOfb5+QDgQQQUwGBWizVmC4rVMsogKQBQ4wgogMEinZExW1AiXZGq1gMA1UJAAQwWag8pnogX9tQ6Fb3GE3GFZtPVGIA30c0YMFy0I6rwkrDSiYvGQemKEE4AeBoBBagBodkhtfW2uV0GxKB5QLUQUADgEjFoHlA93IMCAJeAQfOA6iKgAMAlYNA8oLoIKABwCRg0D6guAgoAXAIGzQOqi4ACAJeAQfOA6iKgVMPXviY9/PD4PvPww4XPATACg+YB1UU342qoq5M2bCj8//r1H//+hx8uvP8f/qGydQEYFwbNA6qHgFINF0LJpYSUi8PJpYQZAFXFoHlAdRBQquVSQgrhBAAASQSU6horpBBOAAAYQUCptnIhhXACAEARAoobLg4pjzwinTtHOAEA4CIB27ZH69nvimw2q3A4rEwmo8bGRrfLqaxgsBBOJk+WBgfdrgYAgMvm9PmbcVDc8vDDH4WTc+fGP04KAAAeRkBxw8X3nAwOFl43bCCkAABwHvegVFu5G2LHM04KAAA+QEBxWColDQyUTm9qkmI7xuitQ0hx3JjbIlb9ei6XV9YDQCn279E5HlB6e3v17//+73rrrbfU0NCgRYsWaePGjYrH404vyjiplBSPS/kyT1236j9U8v+eVWys3jqEFMeMuS0sKZmsjZ3fK+sBoBT799gcvwdl7969WrVqlV599VXt3r1bH374oW655RadPXvW6UUZZ2Cg/BdNkvL/d4UG/t/6jw8d69dzT4oDxtwW+fL/YjGRV9YDQCn277E53oLyox/9qOj35557TjNmzNDBgwf1u7/7uyXvHxwc1OBFXWyz2azTJZnj7rsv7X0XQszQUOVqAQDAYBW/ByWTyUiSpk2bVnZ+b2+vHnrooUqXUXu4vAMA8LGKdjMeHh7W2rVrtXjxYl133XVl39PT06NMJjPy09/fX8mSAABADahoC8qqVav05ptv6pVXXhn1PcFgUMFgsJJlAACAGlOxFpR7771XL7zwgl5++WVdddVVlVqMUZqaCndel2NZhfmoDq9sC6+sB4BS7N9jc7wFxbZt3Xfffdq1a5d+8pOfqLW11elFGCsWK3QLo0+7+7yyLbyyHgBKsX+PzfGHBf7VX/2Vdu7cqe9///tFY5+Ew2E1NDR87Od99bBAAAA8wunzt+MBJRAIlJ2+fft2dXR0fOznCSi1K9eXU3pbWvmjeVktliKdEYXaQ26XBQCoAqfP3xW5xAP/Obb9mJJ3JaWAJFtSQEptSimeiCvaEXW7PABAjeFpxpiwXF+uEE6GJQ2p6DXZlVTuSM7dAgEANYeAgglLb0sXWk7KCUjpRLqq9QAAah8BBROWP5ovXNYpxz4/HwCAcSCgYMKsFmvMFhSrZZSO/gAAjIKAggmLdEbGbEGJdEWqWg8AoPYRUDBhofaQ4ol44dtUp6LXeCKu0Gy6GgMAxqfiTzOGP0Q7ogovCSuduGgclK4I4QQAcFkIKHBMaHZIbb1tbpcBAPAALvEAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMZhoDYAgBFyfTmlt100GnVnRKF2RqP2KwIKAMB1x7YfU/KuZOHJ6LakgJTalFI8EVe0I+p2eXABl3gAAK7K9eUK4WRY0pCKXpNdSeWO5NwtEK4goAAAXJXeli60nJQTkNKJdFXrgRkIKAAAV+WP5guXdcqxz8+H73APClADbNvW+795X2fOndEnJn9C0xumKxAY7Z+cQG2xWqwxW1CsFquq9cAMBBTAYKfyp7Tj8A49ceAJ/fKDX45Mv2bqNbpv/n1aOXelfsf6HfcKBBwQ6YwotSlVfqYtRboi1S0IRgjYtj1aw5orstmswuGwMpmMGhsb3S4HcM1LR17Snzz/J8p9WLhB0L6oDTxw/p+boStC+rfb/03LZi9zpUbAKceeO6ZkV3EvHtmiF08Ncfr8TUABDPTSkZe0fOdy2batYQ2P+r5JmqRAIKAf/tkPCSmoebkjOaUTF42D0hVRaDbjoNQKAorhUilpYKB0elOTFItVv57L5ZX1qEWn8qd01eNX6Tcf/mbMcHLBJE1SwxUN+nX3r31/uccrA315ZT0qiWOUeZw+f3MPioNSKSkel/Jlbji3LCmZrI0dxyvrUat2HN6h3Ie5oks6YxnWsHIf5vTPb/yzVi9YXeHqzOWVgb68sh6VxDHKH+hm7KCBgfI7jFSYXi7tm8gr61GLbNvWEweeuKzPbtm/RYY1iFaNVwb68sp6VBrHKH8goAAGef837+uXH/zykltPLrBl65cf/FInf3OyQpWZzSsDfXllPQAnEFAAg5w5d2ZCnz997rRDldQWrwz05ZX1AJxAQAEM8onJn5jQ56dMnuJQJbXFKwN9eWU9ACcQUACDTG+YrmumXjMyzsmlCiiga6Zeo2kN0ypUmdkinZExWx5qZaAvr6wH4AQCioOamgp3kJdjWYX5tcAr61GLAoGA7pt/32V9dvWC1b4d/j7UHlI8ES8c0epU9BpPxGtmLA2vrEelcYzyB8ZBcZhX+uZ7ZT1q0bjHQQlMUkM946BI3hnoyyvrUUkco8zDQG2AD4x3JNn/XPGfuuWaW6pYIQAUc/r8zSUewEDLZi/TD//sh2q4okGB8/9d7MK0hisaCCcAPImAAhhq2exl+nX3r7X5DzerbWpb0by2qW3a/Ieb9V73e4QTAJ7EJR6gBti2rZO/OanT505ryuQpmtYwzbc3xAIwE8/iAXwoEAhoemi6poemu10KAFQFl3gAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjFPvdgGojlxfTultaeWP5mW1WIp0RhRqD7ldFgAAZRFQfODY9mNK3pWUApJsSQEptSmleCKuaEfU7fIAACjBJR6Py/XlCuFkWNKQil6TXUnljuTcLRAAgDIIKB6X3pYutJyUE5DSiXRV6wEA4FIQUDwufzRfuKxTjn1+PgAAhiGgeJzVYo3ZgmK1WFWtBwCAS0FA8bhIZ2TMFpRIV6Sq9QAAcCkIKB4Xag8pnogXtnSdil7jibhCs+lqDAAwD92MfSDaEVV4SVjpxEXjoHRFCCcAAGMRUHwiNDuktt42t8sAAOCSVOwSz1NPPaWWlhZZlqUFCxbowIEDlVoUAADwmIoElO9+97vq7u7Wgw8+qEOHDumGG27QsmXLdOLEiUosDgAAeEzAtu3R+nhctgULFuhzn/ucnnzySUnS8PCwZs2apfvuu0/r1q0reu/g4KAGBwdHfs9kMorFYurv71djY6PTpQEAgArIZrOaNWuWTp06pXA4POG/5/g9KOfOndPBgwfV09MzMm3SpElaunSp9u3bV/L+3t5ePfTQQyXTZ82a5XRpAACgwt5//30zA8rAwICGhoY0c+bMoukzZ87UW2+9VfL+np4edXd3j/x+6tQpXX311UqlUo6sICbmQiKmRct9bAtzsC3MwbYwx4UrINOmTXPk77neiycYDCoYDJZMD4fDfNkM0tjYyPYwBNvCHGwLc7AtzDFpkjO3tzp+k2xTU5Pq6up0/PjxounHjx9XJMKopQAA4OM5HlAmT56sz372s9qzZ8/ItOHhYe3Zs0cLFy50enEAAMCDKnKJp7u7WytXrtS8efM0f/58bd68WWfPntWdd975sZ8NBoN68MEHy172QfWxPczBtjAH28IcbAtzOL0tKtLNWJKefPJJfeMb31A6ndbcuXO1ZcsWLViwoBKLAgAAHlOxgAIAAHC5eJoxAAAwDgEFAAAYh4ACAACMQ0ABAADGMS6gPPXUU2ppaZFlWVqwYIEOHDjgdkm+09vbq8997nOaMmWKZsyYodtuu03JZNLtsiDp61//ugKBgNauXet2Kb713nvv6c///M81ffp0NTQ06Prrr9frr7/udlm+MzQ0pPXr16u1tVUNDQ265ppr9PDDD4t+H5X305/+VLfeequam5sVCAT0ve99r2i+bdvasGGDotGoGhoatHTpUvX19Y17OUYFlO9+97vq7u7Wgw8+qEOHDumGG27QsmXLdOLECbdL85W9e/dq1apVevXVV7V79259+OGHuuWWW3T27Fm3S/O11157Td/61rf0mc98xu1SfOuDDz7Q4sWLdcUVV+jFF1/Uf//3f+sf//EfNXXqVLdL852NGzfq6aef1pNPPqn/+Z//0caNG7Vp0yY98cQTbpfmeWfPntUNN9ygp556quz8TZs2acuWLXrmmWe0f/9+XXnllVq2bJny+fz4FmQbZP78+faqVatGfh8aGrKbm5vt3t5eF6vCiRMnbEn23r173S7Ft06fPm23t7fbu3fvtn/v937PXrNmjdsl+dIDDzxgL1myxO0yYNv28uXL7c7OzqJpf/zHf2yvWLHCpYr8SZK9a9eukd+Hh4ftSCRif+Mb3xiZdurUKTsYDNr/+q//Oq6/bUwLyrlz53Tw4EEtXbp0ZNqkSZO0dOlS7du3z8XKkMlkJMmxJ1Ri/FatWqXly5cX7R+ovh/84AeaN2+evvjFL2rGjBm68cYb9eyzz7pdli8tWrRIe/bs0dtvvy1JeuONN/TKK6/oC1/4gsuV+du7776rdDpddKwKh8NasGDBuM/lrj/N+IKBgQENDQ1p5syZRdNnzpypt956y6WqMDw8rLVr12rx4sW67rrr3C7Hl77zne/o0KFDeu2119wuxffeeecdPf300+ru7tbf/d3f6bXXXtPq1as1efJkrVy50u3yfGXdunXKZrOaM2eO6urqNDQ0pEcffVQrVqxwuzRfS6fTklT2XH5h3qUyJqDATKtWrdKbb76pV155xe1SfKm/v19r1qzR7t27ZVmW2+X43vDwsObNm6fHHntMknTjjTfqzTff1DPPPENAqbLnn39e3/72t7Vz505de+21Onz4sNauXavm5ma2hUcYc4mnqalJdXV1On78eNH048ePKxKJuFSVv91777164YUX9PLLL+uqq65yuxxfOnjwoE6cOKGbbrpJ9fX1qq+v1969e7VlyxbV19draGjI7RJ9JRqN6tOf/nTRtE996lNKpVIuVeRfX/nKV7Ru3Trdcccduv766/UXf/EXuv/++9Xb2+t2ab524XztxLncmIAyefJkffazn9WePXtGpg0PD2vPnj1auHChi5X5j23buvfee7Vr1y79+Mc/Vmtrq9sl+dbNN9+sX/ziFzp8+PDIz7x587RixQodPnxYdXV1bpfoK4sXLy7pcv/222/r6quvdqki/8rlcpo0qfgUVldXp+HhYZcqgiS1trYqEokUncuz2az2798/7nO5UZd4uru7tXLlSs2bN0/z58/X5s2bdfbsWd15551ul+Yrq1at0s6dO/X9739fU6ZMGbluGA6H1dDQ4HJ1/jJlypSSe3+uvPJKTZ8+nXuCXHD//fdr0aJFeuyxx3T77bfrwIED2rp1q7Zu3ep2ab5z66236tFHH1UsFtO1116rn//853r88cfV2dnpdmmed+bMGR05cmTk93fffVeHDx/WtGnTFIvFtHbtWj3yyCNqb29Xa2ur1q9fr+bmZt12223jW5BDPY0c88QTT9ixWMyePHmyPX/+fPvVV191uyTfkVT2Z/v27W6XBtumm7HL/uM//sO+7rrr7GAwaM+ZM8feunWr2yX5UjabtdesWWPHYjHbsiy7ra3N/vu//3t7cHDQ7dI87+WXXy57jli5cqVt24WuxuvXr7dnzpxpB4NB++abb7aTyeS4lxOwbYbdAwAAZjHmHhQAAIALCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYJz/Dyf07QugJw1oAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at step 28672 to Train\\Saved_Models\\model_step_28672.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 999         |\n",
      "|    ep_rew_mean          | -3.08e+04   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 6           |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 4497        |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045800783 |\n",
      "|    clip_fraction        | 0.446       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 0.907       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.68e+04    |\n",
      "|    n_updates            | 13000       |\n",
      "|    policy_gradient_loss | -0.0861     |\n",
      "|    value_loss           | 1.8e+04     |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1000000, callback=combined_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
