{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "import heapq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGSN_Env(gym.Env):\n",
    "    def __init__(self, grid_size=10, max_static_obstacles=10, max_dynamic_obstacles=6, max_steps=100000):\n",
    "        super(DGSN_Env, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.max_static_obstacles = max_static_obstacles\n",
    "        self.max_dynamic_obstacles = max_dynamic_obstacles\n",
    "        self.num_static_obstacles = np.random.randint(1,10)\n",
    "        self.num_dynamic_obstacles = np.random.randint(1,7)\n",
    "        self.agent_pos = np.array(np.random.uniform(0, self.grid_size, size=2).round(1))\n",
    "        self.goal_pos = np.array(np.random.uniform(0, self.grid_size, size=2).round(1))\n",
    "        self.dynamic_obstacles = self._init_dynamic_obstacles()\n",
    "        self.static_obstacles = self._init_static_obstacles()\n",
    "        \n",
    "        self.dist_factor = 200\n",
    "        self.max_steps = max_steps\n",
    "        self.current_step = 0\n",
    "        \n",
    "        self.action_space = spaces.Discrete(9)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'agent': spaces.Box(low=0, high=grid_size, shape=(2,), dtype=np.float32),\n",
    "            'dyn_obs': spaces.Box(low=0, high=grid_size, shape=(self.max_dynamic_obstacles, 2), dtype=np.float32),\n",
    "            'sta_obs': spaces.Box(low=0, high=grid_size, shape=(self.max_static_obstacles, 2), dtype=np.float32)\n",
    "        })\n",
    "        \n",
    "        print(\"*******************************************************************\\n\")\n",
    "        print(\"Initialized the environment with the following\")\n",
    "        print(\"Agent's Initial Position :\", self.agent_pos)\n",
    "        print(\"Goal Position :\", self.goal_pos)\n",
    "        print(\"Number of Static Obstacles :\", self.num_static_obstacles)\n",
    "        print(\"Static Obstacle Positions :\", self. static_obstacles)\n",
    "        print(\"Number of Dynamic Obstacles :\", self.num_dynamic_obstacles)\n",
    "        print(\"Dynamic Obstacle Positions :\", self. dynamic_obstacles, \"\\n\")\n",
    "        print(\"*******************************************************************\")\n",
    "        \n",
    "    def _init_static_obstacles(self):\n",
    "        obstacles = []\n",
    "        for _ in range(self.num_static_obstacles):\n",
    "            obstacles.append(np.random.uniform(0, self.grid_size, size=2).round(1))\n",
    "        return obstacles\n",
    "    \n",
    "    def _init_dynamic_obstacles(self):\n",
    "        obstacles = []\n",
    "        for _ in range(self.num_dynamic_obstacles):\n",
    "            start_pos = np.random.uniform(0, self.grid_size, size=2).round(1)\n",
    "            end_pos = np.random.uniform(0, self.grid_size, size=2).round(1)\n",
    "            path = self._compute_path(start_pos, end_pos)\n",
    "            obstacles.append({\n",
    "                'start': start_pos,\n",
    "                'end': end_pos,\n",
    "                'current': start_pos.copy(),\n",
    "                'angle': np.rad2deg(np.arctan2(end_pos[1] - start_pos[1], end_pos[0] - start_pos[0])),\n",
    "                'distance': np.linalg.norm(end_pos - start_pos),\n",
    "                'path': path if path else []\n",
    "            })\n",
    "        return obstacles\n",
    "    \n",
    "    def _compute_path(self, start, end):\n",
    "        def heuristic(a, b):\n",
    "            return np.linalg.norm(a - b)\n",
    "\n",
    "        def a_star(start, goal):\n",
    "            open_set = []\n",
    "            heapq.heappush(open_set, (0, tuple(start)))\n",
    "            came_from = {}\n",
    "            g_score = {tuple(start): 0}\n",
    "            f_score = {tuple(start): heuristic(start, goal)}\n",
    "\n",
    "            while open_set:\n",
    "                _, current = heapq.heappop(open_set)\n",
    "                current = np.array(current)\n",
    "\n",
    "                if np.array_equal(current, goal):\n",
    "                    path = []\n",
    "                    while tuple(current) in came_from:\n",
    "                        path.append(current)\n",
    "                        current = came_from[tuple(current)]\n",
    "                    path.append(start)\n",
    "                    path.reverse()\n",
    "                    return path\n",
    "\n",
    "                neighbors = [current + [0.1, 0], current + [-0.1, 0], current + [0, 0.1], current + [0, -0.1]]\n",
    "                neighbors = [np.clip(neighbor, 0, self.grid_size - 0.5) for neighbor in neighbors]\n",
    "\n",
    "                for neighbor in neighbors:\n",
    "                    tentative_g_score = g_score[tuple(current)] + heuristic(current, neighbor)\n",
    "                    if tuple(neighbor) not in g_score or tentative_g_score < g_score[tuple(neighbor)]:\n",
    "                        came_from[tuple(neighbor)] = current\n",
    "                        g_score[tuple(neighbor)] = tentative_g_score\n",
    "                        f_score[tuple(neighbor)] = tentative_g_score + heuristic(neighbor, goal)\n",
    "                        heapq.heappush(open_set, (f_score[tuple(neighbor)], tuple(neighbor)))\n",
    "\n",
    "            return []\n",
    "\n",
    "        return a_star(start, end)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        previous_agent_pos = self.agent_pos.copy()\n",
    "        print(f\"Agent's Current Position {previous_agent_pos}\")\n",
    "        \n",
    "        if action == 0:\n",
    "            self.agent_pos[1] += 0.1\n",
    "            print(\"Action Taken : Up\")\n",
    "        elif action == 1:\n",
    "            self.agent_pos[1] -= 0.1\n",
    "            print(\"Action Taken : Down\")\n",
    "        elif action == 2:\n",
    "            self.agent_pos[0] -= 0.1\n",
    "            print(\"Action Taken : Left\")\n",
    "        elif action == 3:\n",
    "            self.agent_pos[0] += 0.1\n",
    "            print(\"Action Taken : Right\")\n",
    "        elif action == 4:\n",
    "            self.agent_pos += [0.1, 0.1]\n",
    "            print(\"Action Taken : Right-Up\")\n",
    "        elif action == 5:\n",
    "            self.agent_pos += [-0.1, 0.1]\n",
    "            print(\"Action Taken : Left-Up\")\n",
    "        elif action == 6:\n",
    "            self.agent_pos += [0.1, -0.1]\n",
    "            print(\"Action Taken : Right-Down\")\n",
    "        elif action == 7:\n",
    "            self.agent_pos += [-0.1, -0.1]\n",
    "            print(\"Action Taken : Left-Down\")\n",
    "        elif action == 8:\n",
    "            print(\"Action Taken : Stay Still!\")\n",
    "            pass\n",
    "        \n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size - 0.5)\n",
    "        print(f\"Agent's New Position {self.agent_pos}\")\n",
    "        \n",
    "        for obstacle in self.dynamic_obstacles:\n",
    "            if len(obstacle['path']) > 1:\n",
    "                obstacle['current'] = obstacle['path'].pop(0)\n",
    "            else:\n",
    "                obstacle['path'] = self._compute_path(obstacle['start'], obstacle['end'])\n",
    "        \n",
    "        reward = self._compute_reward(previous_agent_pos)\n",
    "        print(f\"Reward Obtain : {reward}\")\n",
    "        done = self._is_done()\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "            reward -= 50\n",
    "        truncated = False\n",
    "        \n",
    "        return self._get_obs(), reward, done, truncated, {}\n",
    "    \n",
    "    def _compute_reward(self, previous_agent_pos):\n",
    "        previous_dist_to_goal = np.linalg.norm(previous_agent_pos - self.goal_pos)\n",
    "        current_dist_to_goal = np.linalg.norm(self.agent_pos - self.goal_pos)\n",
    "        reward = (current_dist_to_goal - previous_dist_to_goal) * self.dist_factor\n",
    "        \n",
    "        # Check for collisions with obstacles and boundaries\n",
    "        for obs in self.dynamic_obstacles:\n",
    "            angle = np.rad2deg(np.arctan2(obs['current'][1] - self.agent_pos[1], obs['current'][0] - self.agent_pos[0]))\n",
    "            distance = np.linalg.norm(obs['current'] - self.agent_pos)\n",
    "            # Update reward based on obstacle proximity\n",
    "            # reward -= distance*self.dist_factor  #\n",
    "        if distance < 0.5:  # Define a threshold for collision\n",
    "            reward -= 10*(1/distance)  # Penalty for colliding with a dynamic obstacle\n",
    "        if distance == 0:\n",
    "            reward -= 125\n",
    "        if any(np.array_equal(self.agent_pos, obs) for obs in self.dynamic_obstacles):\n",
    "            reward -= 15  # Penalty for colliding with a Dynamic obstacle\n",
    "        if any(np.array_equal(self.agent_pos, obs) for obs in self.static_obstacles):\n",
    "            reward -= 15  # Penalty for colliding with a Static obstacle\n",
    "        if np.any(self.agent_pos == 0) or np.any(self.agent_pos >= self.grid_size - 0.5):\n",
    "            reward -= 5  # Penalty for hitting the wall/boundary\n",
    "            \n",
    "        reward -= 1\n",
    "        return reward\n",
    "    \n",
    "    def _is_done(self):\n",
    "        return np.array_equal(self.agent_pos, self.goal_pos)\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        agent_state = np.array([self.agent_pos[0], self.agent_pos[1]], dtype=np.float32)\n",
    "        dynamic_obstacle_states = np.array([(ob['angle'], ob['distance']) for ob in self.dynamic_obstacles] + \n",
    "                                           [np.zeros(2) for _ in range(self.max_dynamic_obstacles - len(self.dynamic_obstacles))],\n",
    "                                           dtype=np.float32)\n",
    "        static_obstacle_states = np.array([ob[:2] for ob in self.static_obstacles] + \n",
    "                                          [np.zeros(2) for _ in range(self.max_static_obstacles - len(self.static_obstacles))],\n",
    "                                          dtype=np.float32)\n",
    "        return {\n",
    "            'agent': agent_state,\n",
    "            'dyn_obs': dynamic_obstacle_states,\n",
    "            'sta_obs': static_obstacle_states\n",
    "        }\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if not hasattr(self, 'fig'):\n",
    "            self.fig, self.ax = plt.subplots()\n",
    "            self.ax.set_xlim(0, self.grid_size)\n",
    "            self.ax.set_ylim(0, self.grid_size)\n",
    "            # Initializing markers with initial positions\n",
    "            self.agent_marker, = self.ax.plot(self.agent_pos[0], self.agent_pos[1], 'go', markersize=10)\n",
    "            self.goal_marker, = self.ax.plot(self.goal_pos[0], self.goal_pos[1], 'rx', markersize=10)\n",
    "            self.dynamic_markers = [self.ax.plot(ob['current'][0], ob['current'][1], 'mo', markersize=5)[0] for ob in self.dynamic_obstacles]\n",
    "            self.static_markers = [self.ax.plot(ob[0], ob[1], 'bs', markersize=5)[0] for ob in self.static_obstacles]\n",
    "        else:\n",
    "            # Ensuring All positions are sequences\n",
    "            self.agent_marker.set_data([self.agent_pos[0]], [self.agent_pos[1]])\n",
    "            self.goal_marker.set_data([self.goal_pos[0]], [self.goal_pos[1]])\n",
    "            for marker, obstacle in zip(self.dynamic_markers, self.dynamic_obstacles):\n",
    "                marker.set_data([obstacle['current'][0]], [obstacle['current'][1]])\n",
    "            for marker, obstacle in zip(self.static_markers, self.static_obstacles):\n",
    "                marker.set_data([obstacle[0]], [obstacle[1]])\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        display(self.fig)\n",
    "        plt.pause(0.01)  # A short pause to update the plot\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        if 'seed' in kwargs:\n",
    "            np.random.seed(kwargs['seed'])\n",
    "\n",
    "        self.agent_pos = np.random.uniform(0, self.grid_size, size=2).round(2)\n",
    "        self.goal_pos = np.random.uniform(0, self.grid_size, size=2).round(2)\n",
    "        self.dynamic_obstacles = self._init_dynamic_obstacles()\n",
    "        self.static_obstacles = self._init_static_obstacles()\n",
    "        self.current_step = 0\n",
    "        return self._get_obs(), {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class RenderCallback(BaseCallback):\n",
    "    def __init__(self, env, render_freq=1, verbose=0):\n",
    "        super(RenderCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.render_freq = render_freq\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.render_freq == 0:\n",
    "            self.env.render()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DGSN_Env()\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Train','Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_callback = RenderCallback(env, render_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MultiInputPolicy\", env, verbose=1, tensorboard_log=log_path, device='cuda', n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=10000, callback=render_callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
