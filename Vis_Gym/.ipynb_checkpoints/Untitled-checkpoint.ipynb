{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21420e80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the engine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n",
      "Initialised\n",
      "initialize_robot_and_goal\n",
      "_find_free_space\n",
      "_find_free_space\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamolharsh123\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/apple/Library/CloudStorage/OneDrive-PlakshaUniversity/CustomGym/Vis_Gym/wandb/run-20240627_181529-ptd2cvwm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amolharsh123/DGSN_Grid_World/runs/ptd2cvwm' target=\"_blank\">fast-music-5</a></strong> to <a href='https://wandb.ai/amolharsh123/DGSN_Grid_World' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/amolharsh123/DGSN_Grid_World' target=\"_blank\">https://wandb.ai/amolharsh123/DGSN_Grid_World</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/amolharsh123/DGSN_Grid_World/runs/ptd2cvwm' target=\"_blank\">https://wandb.ai/amolharsh123/DGSN_Grid_World/runs/ptd2cvwm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRINTING & LOGGING!!! 1\n",
      "initialize_robot_and_goal\n",
      "_find_free_space\n",
      "_find_free_space\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "PRINTING & LOGGING!!! 2\n",
      "initialize_robot_and_goal\n",
      "_find_free_space\n",
      "_find_free_space\n",
      "Logging to Train/Logs/PPO_12\n",
      "Model saved at step 100 to Train/Saved_Models/PPO_Grid_1/model_step_100.zip\n",
      "Model saved at step 200 to Train/Saved_Models/PPO_Grid_1/model_step_200.zip\n",
      "Model saved at step 300 to Train/Saved_Models/PPO_Grid_1/model_step_300.zip\n",
      "Model saved at step 400 to Train/Saved_Models/PPO_Grid_1/model_step_400.zip\n",
      "Model saved at step 500 to Train/Saved_Models/PPO_Grid_1/model_step_500.zip\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'10'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 552\u001b[0m\n\u001b[1;32m    549\u001b[0m combined_callback \u001b[38;5;241m=\u001b[39m CustomCallback(env\u001b[38;5;241m=\u001b[39menv, render_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, save_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, save_path\u001b[38;5;241m=\u001b[39msave_path, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    550\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiInputPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, tensorboard_log\u001b[38;5;241m=\u001b[39mlog_path, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m--> 552\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ilgc/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ilgc/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:277\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 277\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/ilgc/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:194\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 194\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/ilgc/lib/python3.11/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ilgc/lib/python3.11/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/ilgc/lib/python3.11/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[0;32mIn[1], line 267\u001b[0m, in \u001b[0;36mDGSN_Env.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, obstacle \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_obstacles):\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obstacle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 267\u001b[0m         obstacle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcurrent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mobstacles_positions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m#print(f\"#####################################\")\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m#print(f\"Reward Obtained : {reward}\")\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m#print(f\"#####################################\")\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[0;31mKeyError\u001b[0m: '10'"
     ]
    }
   ],
   "source": [
    "#custom files\n",
    "import gridworld_converter\n",
    "import robo_nav_algo as robo\n",
    "import free_space_finder\n",
    "import utils\n",
    "import obstacle_generator as obs\n",
    "import baseline_robo_nav as baseline\n",
    "\n",
    "#standard files\n",
    "# %matplotlib notebook\n",
    "import cv2\n",
    "\n",
    "# Load the Image\n",
    "image = cv2.imread('example6.png', cv2.IMREAD_COLOR)\n",
    "# 2d matrix representation of the above blueprint\n",
    "# value of scale can be used to control how much you want to zoom into the grid world\n",
    "gridworld = gridworld_converter.grid_convert('example6.png',scale = 3)\n",
    "#print(f\"Number of rows: {gridworld.shape[0]}\\nNumber of columns: {gridworld.shape[1]}\")\n",
    "\n",
    "number_of_obstacles = 25\n",
    "episode_length = 10000\n",
    "total_timesteps = 10000000  # Total training steps\n",
    "\n",
    "# Initialize the pathfinding class\n",
    "pathfinder = obs.Pathfinding(gridworld)\n",
    "# Generate paths for obstacles\n",
    "obstacle_paths = pathfinder.generate_paths(number_of_obstacles, episode_length)\n",
    "#contains a dictionary of occupied coordinate as keys at different time step as value\n",
    "'only one object can occupy a coordinate at a particular time stamp'\n",
    "occupied_time_steps = pathfinder.occupied_time_steps\n",
    "#contains a list of start or end coordinates of each dynamic obstacle\n",
    "obstacle_occupied_points = pathfinder.obstacle_occupied_points\n",
    "print(\"Starting the engine\")\n",
    "'''\n",
    "Help:\n",
    "You can see the obstacle path by obstacle_paths['1'] or obstacle_paths['2']\n",
    "'''\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "import obstacle_generator as observ\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "import heapq\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "# import matplotlib.pyplot as plt\n",
    "# from IPython.display import display, clear_output\n",
    "\n",
    "class DGSN_Env(gym.Env):\n",
    "    def __init__(self, gridworld, obstacle_paths, occupied_time_steps, obstacle_occupied_points, number_of_obstacles, episode_length):\n",
    "        super(DGSN_Env, self).__init__()\n",
    "        print(\"Initialised\")\n",
    "        #print(f\"It begins!\")\n",
    "        self.gridworld = gridworld\n",
    "        self.grid_size = gridworld.shape\n",
    "        \n",
    "        self.obstacle_paths = obstacle_paths\n",
    "        self.occupied_time_steps = occupied_time_steps\n",
    "        self.obstacle_occupied_points = obstacle_occupied_points\n",
    "        self.episode_length = episode_length\n",
    "        self.boundary_threshold = 0\n",
    "        self.max_dynamic_obstacles = number_of_obstacles\n",
    "        self.num_static_obstacles = 0\n",
    "        self.max_static_obstacles = 0\n",
    "        self.number_of_obstacles = number_of_obstacles\n",
    "        self.num_dynamic_obstacles = number_of_obstacles\n",
    "        self.robot_position = None\n",
    "        self.goal_position = None\n",
    "        self.agent_pos = self.robot_position\n",
    "        self.goal_pos = self.goal_position\n",
    "        self.ep_no = 0\n",
    "        self.global_obstacle_position = []\n",
    "        self.close_call, self.discomfort, self.current_step, self.ep_no = 0, 0, 1, 0\n",
    "        self.dist_factor = 4\n",
    "        self.thresh = 1.5\n",
    "        self.max_steps = episode_length\n",
    "        self.total_reward = 0\n",
    "        self.robot_path = []\n",
    "\n",
    "        self.action_space = spaces.Discrete(9)  # 9 possible actions including no movement\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'agent': spaces.Box(low=0, high=max(self.grid_size), shape=(2,), dtype=np.int32),\n",
    "            'dyn_obs': spaces.Box(low=0, high=max(self.grid_size), shape=(number_of_obstacles, 2), dtype=np.int32)\n",
    "        })\n",
    "        \n",
    "        self._initialize_robot_and_goal()\n",
    "        self.dynamic_obstacles = self._init_dynamic_obstacles(obstacle_paths)\n",
    "        \n",
    "        wandb.login()\n",
    "        wandb.init(project='DGSN_Grid_World')\n",
    "        \n",
    "        self.safe = True\n",
    "        \n",
    "        # Evaluation Metrics\n",
    "        self.Avg_Success_Rate = 0\n",
    "        self.Avg_Collision_Rate, self.ep_collision_rate, self.collision_count = 0, 0, 0\n",
    "        self.Avg_Min_Time_To_Collision, self.min_time_to_collision = 0, 0\n",
    "        self.Avg_Wall_Collision_Rate, self.ep_wall_collision_rate, self.wall_collision_count = 0, 0, 0\n",
    "        self.Avg_Obstacle_Collision_Rate, self.ep_obstacle_collision_rate, self.obstacle_collision_count = 0, 0, 0\n",
    "        self.Avg_Human_Collision_Rate, self.ep_human_collision_rate, self.human_collision_count = 0, 0, 0\n",
    "        self.Avg_Timeout = 0\n",
    "        self.Avg_Path_Length = 0\n",
    "        self.Avg_Stalled_Time, self.stalled_time = 0, 0\n",
    "        self.Avg_Group_Inhibition_Rate, self.ep_group_inhibition_rate, self.group_inhibition = 0, 0, 0\n",
    "        self.Avg_Discomfort, self.ep_discomfort = 0, 0\n",
    "        self.Avg_Human_Distance, self.ep_human_distance, self.human_distance = 0, 0, 0\n",
    "        self.Avg_Closest_Human_Distance, self.closest_human_distance = 0, 0\n",
    "        self.Min_Closest_Human_Distance = 0\n",
    "        self.goal_reached = 0\n",
    "        self.timeout = 0\n",
    "        \n",
    "    def _initialize_robot_and_goal(self):\n",
    "        print(\"initialize_robot_and_goal\")\n",
    "        # Find a free space for the robot start point\n",
    "        self.robot_position = self._find_free_space()\n",
    "        self.agent_pos = self.robot_position\n",
    "        # Find a free space for the goal point\n",
    "        self.goal_position = self._find_free_space()\n",
    "        self.goal_pos = self.goal_position\n",
    "        self.robot_path.append(self.robot_position) \n",
    "        \n",
    "    def _find_free_space(self):\n",
    "        print(\"_find_free_space\")\n",
    "        free_spaces = np.argwhere(self.gridworld == 0)\n",
    "        while True:\n",
    "            candidate = tuple(free_spaces[np.random.choice(len(free_spaces))])\n",
    "            if self._is_free_space(candidate):\n",
    "                return candidate\n",
    "\n",
    "    def _is_free_space(self, position):\n",
    "        \n",
    "        # Check if the position is occupied by any obstacle at the current time step\n",
    "        if position in self.occupied_time_steps and self.occupied_time_steps[position] == self.current_step:\n",
    "            return False\n",
    "        # Check if the position is occupied by the robot or goal\n",
    "        if position == self.robot_position or position == self.goal_position:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def _init_dynamic_obstacles(self, obstacle_paths):\n",
    "        obstacles = []\n",
    "        for i in range(self.number_of_obstacles):\n",
    "            start_pos = obstacle_paths[str(i+1)][0]\n",
    "            end_pos = obstacle_paths[str(i+1)][-1]\n",
    "            path = obstacle_paths[str(i+1)]\n",
    "            distance = np.linalg.norm(np.array(self.agent_pos) - np.array(start_pos))\n",
    "            obstacles.append({\n",
    "                'start': start_pos,\n",
    "                'end': end_pos,\n",
    "                'current': start_pos,\n",
    "                'angle': np.rad2deg(np.arctan2(self.agent_pos[1] - start_pos[1], self.agent_pos[0] - start_pos[0])).astype('float'),\n",
    "                'distance': int(distance),\n",
    "                'path': path if path else []\n",
    "            })\n",
    "        return obstacles\n",
    "    \n",
    "    def _check_goal_reached(self, position):\n",
    "        return position == self.goal_position\n",
    "    \n",
    "    def _is_valid_move(self, position):\n",
    "        \"\"\"\n",
    "        Check if the position is within grid bounds, is a free space (value 0), and is not occupied by any obstacle.\n",
    "        \"\"\"\n",
    "    #         #print(f\"-----------checking if valid move---{position}-------\")\n",
    "        rows, cols = self.gridworld.shape\n",
    "        if not (0 <= position[0] < rows and 0 <= position[1] < cols):\n",
    "    #             #print(\"going outside the world\")\n",
    "            return False\n",
    "        if self.gridworld[position[0], position[1]] != 0:\n",
    "    #             #print(\"Collison with the wall\")\n",
    "            return False\n",
    "        if position in self.occupied_time_steps and self.occupied_time_steps[position] == self.current_step:\n",
    "    #             #print(\"collision with an obstacle\")\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def _compute_path(self, start, end):\n",
    "        def heuristic(a, b):\n",
    "            return np.linalg.norm(a - b)\n",
    "\n",
    "        def a_star(start, goal):\n",
    "            print(\"path finding\")\n",
    "            open_set = []\n",
    "            heapq.heappush(open_set, (0, tuple(start)))\n",
    "            came_from = {}\n",
    "            g_score = {tuple(start): 0}\n",
    "            f_score = {tuple(start): heuristic(start, goal)}\n",
    "\n",
    "            while open_set:\n",
    "                _, current = heapq.heappop(open_set)\n",
    "                current = np.array(current)\n",
    "\n",
    "                if np.array_equal(current, goal):\n",
    "                    path = []\n",
    "                    while tuple(current) in came_from:\n",
    "                        path.append(current)\n",
    "                        current = came_from[tuple(current)]\n",
    "                    path.append(start)\n",
    "                    path.reverse()\n",
    "                    return path\n",
    "                \n",
    "                val = 1\n",
    "                neighbors = [current + [val, 0], current + [-val, 0], current + [0, val], current + [0, -val]]\n",
    "                neighbors = [np.clip(neighbor, 0, self.grid_size - 0.5) for neighbor in neighbors]\n",
    "\n",
    "                for neighbor in neighbors:\n",
    "                    tentative_g_score = g_score[tuple(current)] + heuristic(current, neighbor)\n",
    "                    if tuple(neighbor) not in g_score or tentative_g_score < g_score[tuple(neighbor)]:\n",
    "                        came_from[tuple(neighbor)] = current\n",
    "                        g_score[tuple(neighbor)] = tentative_g_score\n",
    "                        f_score[tuple(neighbor)] = tentative_g_score + heuristic(neighbor, goal)\n",
    "                        heapq.heappush(open_set, (f_score[tuple(neighbor)], tuple(neighbor)))\n",
    "            return []\n",
    "\n",
    "        return a_star(start, end)\n",
    "    \n",
    "    def _get_obstacles_positions(self):\n",
    "        obstacles_positions = {}\n",
    "        for key, path in self.obstacle_paths.items():\n",
    "            if self.current_step < len(path):\n",
    "                obstacles_positions[key] = path[self.current_step]\n",
    "        return obstacles_positions\n",
    "    \n",
    "    def _move(self, position, action):\n",
    "        directions = [\n",
    "            (0, 1), (1, 0), (0, -1), (-1, 0),  # Right, Down, Left, Up\n",
    "            (-1, 1), (-1, -1), (1, 1), (1, -1),(0,0)  # Top-Right, Top-Left, Bottom-Right, Bottom-Left, No movement\n",
    "        ]\n",
    "        direction = directions[action]\n",
    "        new_position = (position[0] + direction[0], position[1] + direction[1])\n",
    "        return new_position\n",
    "    \n",
    "    def step(self, action):\n",
    "        previous_agent_pos = self.robot_position\n",
    "        new_position = self._move(self.robot_position, action)\n",
    "        collision = not self._is_valid_move(new_position)\n",
    "        goal_reached = self._check_goal_reached(new_position)\n",
    "        \n",
    "        if not collision:\n",
    "            self.robot_position = new_position\n",
    "            self.agent_pos = new_position\n",
    "            obstacles_positions = self._get_obstacles_positions()\n",
    "            self.global_obstacle_position = obstacles_positions\n",
    "            self.current_step += 1\n",
    "            self.robot_path.append(new_position)\n",
    "        else:\n",
    "            new_position = previous_agent_pos\n",
    "            obstacles_positions = self.global_obstacle_position\n",
    "            \n",
    "        obs = self._get_obs()\n",
    "        reward = self._compute_reward(previous_agent_pos)\n",
    "        done = goal_reached or self.current_step >= self.episode_length\n",
    "        truncated = self.current_step >= self.episode_length\n",
    "        \n",
    "        #print(f\"Agent's Current Position {previous_agent_pos}\")\n",
    "        #print(f\"Goal Position : {self.goal_pos}\")\n",
    "        #print(f\"Agent's New Position {self.agent_pos}\")\n",
    "        #print(f\"Steps taken : {self.current_step}\")\n",
    "        \n",
    "        \n",
    "        print(f\"Step function called\\nvalue of obstacles_positions: {obstacles_positions.keys()}\")\n",
    "        for i, obstacle in enumerate(self.dynamic_obstacles):\n",
    "            if len(obstacle['path']) > 1:\n",
    "                obstacle['current'] = obstacles_positions[str(i+1)]\n",
    "        \n",
    "        #print(f\"#####################################\")\n",
    "        #print(f\"Reward Obtained : {reward}\")\n",
    "        #print(f\"#####################################\")\n",
    "        \n",
    "        self.total_reward += reward\n",
    "        wandb.log({\"Reward\": reward, \"Total_Reward\": self.total_reward})\n",
    "        \n",
    "        self.done = self._is_done()\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.done = True\n",
    "            reward -= 2500\n",
    "            self.timeout += 1\n",
    "            wandb.log({\"TimeOut\": self.timeout})\n",
    "        \n",
    "        return obs, reward, self.done, truncated, {}\n",
    "\n",
    "    def _compute_reward(self, previous_agent_pos):\n",
    "        reward_c = 0\n",
    "        if self.safe:\n",
    "            self.min_time_to_collision += 1\n",
    "        previous_dist_to_goal = np.linalg.norm(np.array(previous_agent_pos) - np.array(self.goal_pos))\n",
    "        current_dist_to_goal = np.linalg.norm(np.array(self.agent_pos) - np.array(self.goal_pos))\n",
    "        del_distance = current_dist_to_goal - previous_dist_to_goal\n",
    "\n",
    "        #print(f\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% \\n\")\n",
    "        #print(f\"Distance to the Goal : {current_dist_to_goal}\")\n",
    "        #print(f\"Del_Distance : {del_distance}\")\n",
    "\n",
    "        if del_distance > 0:\n",
    "            reward_c -= del_distance * self.dist_factor\n",
    "        elif del_distance == 0:\n",
    "            reward_c = -2\n",
    "        else:\n",
    "            reward_c += (-del_distance) * self.dist_factor * 2\n",
    "\n",
    "        #print(f\"Reward from Del_Dist : {reward_c}\")\n",
    "\n",
    "        for i, obs in enumerate(self.dynamic_obstacles):\n",
    "            distance = np.linalg.norm(np.array(obs['current']) - np.array(self.agent_pos))\n",
    "            self.human_distance += distance\n",
    "            #print(f\"******************************\")\n",
    "            #print(f\"Obstacle #{i} Distance : {distance}\")\n",
    "\n",
    "            if distance < 3 and distance != 0:  \n",
    "                self.closest_human_distance = min(self.closest_human_distance, distance)\n",
    "                self.close_call += 1\n",
    "                pen_1 = (10 / distance) * 2\n",
    "                reward_c -= pen_1  \n",
    "                #print(f\"Penalty Obtained : {pen_1}\")\n",
    "\n",
    "        #print(f\"Reward Post Dynamic Manouevres : {reward_c}\")\n",
    "        #print(f\"******************************\")\n",
    "\n",
    "        grouped_obstacles = self._group_dynamic_obstacles()\n",
    "        for j, group in enumerate(grouped_obstacles):\n",
    "            group_center = np.mean(group, axis=0)\n",
    "            if np.linalg.norm(np.array(self.agent_pos) - group_center) < self.thresh:                \n",
    "                reward_c -= 50\n",
    "                self.group_inhibition += 1\n",
    "                self.close_call += 1\n",
    "                #print(f\"Oops Inhibited the Group {j} - Penalty -50\")\n",
    "                wandb.log({\"Group_Inhibition\": self.group_inhibition})\n",
    "\n",
    "        if any(np.array_equal(self.agent_pos, np.array(obs['current'])) for obs in self.dynamic_obstacles):\n",
    "            self.collision_count += 1\n",
    "            self.human_collision_count += 1\n",
    "            self.safe = False\n",
    "            reward_c -= 30  \n",
    "            #print(f\"Collided with a Human!!!\")\n",
    "            #print(f\"Post Human Collision Reward : {reward_c}\")\n",
    "\n",
    "        if np.any(np.array(self.agent_pos) <= self.boundary_threshold) or np.any(np.array(self.agent_pos) >= np.array(self.grid_size) - self.boundary_threshold):\n",
    "            reward_c -= 15  \n",
    "            #print(f\"Pretty Close to the Boundary!!!\")\n",
    "            #print(f\"Post Boundary Penalty Reward : {reward_c}\")\n",
    "\n",
    "        if np.any(np.array(self.agent_pos) == 0) or np.any(np.array(self.agent_pos) >= np.array(self.grid_size)):\n",
    "            self.collision_count += 1\n",
    "            self.wall_collision_count += 1\n",
    "            self.safe = False\n",
    "            reward_c -= 20  \n",
    "            #print(f\"Collided with the wall!!!\")\n",
    "            #print(f\"Post Wall Collision Reward : {reward_c}\")\n",
    "\n",
    "        reward_c -= 1\n",
    "\n",
    "        if self._is_done():\n",
    "            reward_c += 3000\n",
    "            #print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \\n\")\n",
    "            #print(f\"Goal Reached!!!!!\")\n",
    "            #print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \\n\")\n",
    "            self.goal_reached += 1\n",
    "            wandb.log({'Goal_Reached': self.goal_reached})\n",
    "\n",
    "        return reward_c\n",
    "\n",
    "    \n",
    "    def _is_done(self):\n",
    "        return np.array_equal(np.array(self.agent_pos), np.array(self.goal_pos))\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        agent_state = np.array([self.agent_pos[0], self.agent_pos[1]], dtype=np.int32)\n",
    "        \n",
    "        dynamic_obstacle_states = np.array(\n",
    "            [(np.rad2deg(np.arctan2(self.agent_pos[1] - ob['current'][1], self.agent_pos[0] - ob['current'][0])), \n",
    "              np.linalg.norm(np.array(self.agent_pos) - np.array(ob['current']))) \n",
    "             for ob in self.dynamic_obstacles] + \n",
    "            [np.zeros(2, dtype=np.int32) for _ in range(self.max_dynamic_obstacles - len(self.dynamic_obstacles))], \n",
    "            dtype=np.int32)\n",
    "\n",
    "        return {\n",
    "            'agent': agent_state,\n",
    "            'dyn_obs': dynamic_obstacle_states\n",
    "        }\n",
    "    \n",
    "\n",
    "        \n",
    "    def _group_dynamic_obstacles(self):\n",
    "        positions = np.array([np.array(obs['current']) for obs in self.dynamic_obstacles])\n",
    "        clustering = DBSCAN(eps=0.3, min_samples=2).fit(positions)\n",
    "        labels = clustering.labels_\n",
    "\n",
    "        grouped_obstacles = []\n",
    "        for label in set(labels):\n",
    "            if label != -1:\n",
    "                group = positions[labels == label]\n",
    "                grouped_obstacles.append(group)\n",
    "        return grouped_obstacles\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        \n",
    "        wandb.log({\"Episode\": self.ep_no})\n",
    "        self.ep_no += 1\n",
    "        print(f\"PRINTING & LOGGING!!! {self.ep_no}\")\n",
    "\n",
    "        if self.current_step > 0:\n",
    "            self.ep_human_distance = self.human_distance / (self.num_dynamic_obstacles * self.current_step)\n",
    "            self.ep_discomfort = self.close_call / self.current_step\n",
    "            self.ep_collision_rate = self.collision_count / self.current_step\n",
    "            self.ep_wall_collision_rate = self.wall_collision_count / self.current_step\n",
    "            self.ep_obstacle_collision_rate = self.obstacle_collision_count / self.current_step\n",
    "            self.ep_human_collision_rate = self.human_collision_count / self.current_step\n",
    "            self.ep_group_inhibition_rate = self.group_inhibition / self.current_step\n",
    "        else:\n",
    "            self.ep_human_distance = 0\n",
    "            self.ep_discomfort = 0\n",
    "            self.ep_collision_rate = 0\n",
    "            self.ep_wall_collision_rate = 0\n",
    "            self.ep_obstacle_collision_rate = 0\n",
    "            self.ep_human_collision_rate = 0\n",
    "            self.ep_group_inhibition_rate = 0\n",
    "\n",
    "        self.Avg_Collision_Rate = ((self.ep_no - 1) * self.Avg_Collision_Rate + self.ep_collision_rate) / self.ep_no\n",
    "        self.Avg_Min_Time_To_Collision = ((self.ep_no - 1) * self.Avg_Min_Time_To_Collision + self.min_time_to_collision) / self.ep_no\n",
    "        self.Avg_Wall_Collision_Rate = ((self.ep_no - 1) * self.Avg_Wall_Collision_Rate + self.ep_wall_collision_rate) / self.ep_no\n",
    "        self.Avg_Obstacle_Collision_Rate = ((self.ep_no - 1) * self.Avg_Obstacle_Collision_Rate + self.ep_obstacle_collision_rate) / self.ep_no\n",
    "        self.Avg_Human_Collision_Rate = ((self.ep_no - 1) * self.Avg_Human_Collision_Rate + self.ep_human_collision_rate) / self.ep_no\n",
    "        self.Avg_Path_Length = ((self.ep_no - 1) * self.Avg_Path_Length + self.current_step) / self.ep_no\n",
    "        self.Avg_Stalled_Time = ((self.ep_no - 1) * self.Avg_Stalled_Time + self.stalled_time) / self.ep_no\n",
    "        self.Avg_Discomfort = ((self.ep_no - 1) * self.Avg_Discomfort + self.ep_discomfort) / self.ep_no\n",
    "        self.Avg_Human_Distance = ((self.ep_no - 1) * self.Avg_Human_Distance + self.ep_human_distance) / self.ep_no\n",
    "        self.Avg_Closest_Human_Distance = ((self.ep_no - 1) * self.Avg_Closest_Human_Distance + self.closest_human_distance) / self.ep_no\n",
    "        self.Avg_Group_Inhibition_Rate = ((self.ep_no - 1) * self.Avg_Group_Inhibition_Rate + self.ep_group_inhibition_rate) / self.ep_no\n",
    "        \n",
    "        wandb.log({\n",
    "            \"Ep_Total_Reward\": self.total_reward, \n",
    "            \"Ep_Collision_Count\": self.collision_count, \"Ep_Min_Time_To_Collision\": self.min_time_to_collision, \n",
    "            \"Ep_Wall_Collision_Count\": self.wall_collision_count, \"Ep_Obstacle_Collision_Count\": self.obstacle_collision_count, \n",
    "            \"Ep_Human_Collision_Count\": self.human_collision_count, \"Ep_Collision_Rate\": self.ep_collision_rate,\n",
    "            \"Ep_Wall_Collision_Rate\": self.ep_wall_collision_rate, \"Ep_Obstacle_Collision_Rate\": self.ep_obstacle_collision_rate,\n",
    "            \"Ep_Human_Collision_Rate\": self.ep_human_collision_rate, \"Ep_Path_Length\": self.current_step,\n",
    "            \"Ep_Stalled_Time\": self.stalled_time, \"Ep_Discomfort\": self.ep_discomfort,\n",
    "            \"Ep_Avg_Human_Distance\": self.ep_human_distance, \"Ep_Closest_Human_Distance\": self.closest_human_distance,\n",
    "            \"Ep_Close_Calls\": self.close_call, \"Ep_Group_Inhibition\": self.ep_group_inhibition_rate,\n",
    "            \"Avg_Collision_Rate\": self.Avg_Collision_Rate, \"Avg_Min_Time_To_Collision\": self.Avg_Min_Time_To_Collision,\n",
    "            \"Avg_Wall_Collision_Rate\": self.Avg_Wall_Collision_Rate, \"Avg_Obstacle_Collision_Rate\": self.Avg_Obstacle_Collision_Rate,\n",
    "            \"Avg_Human_Collision_Rate\": self.Avg_Human_Collision_Rate, \"Avg_Path_Length\": self.Avg_Path_Length,\n",
    "            \"Avg_Stalled_Time\": self.Avg_Stalled_Time, \"Avg_Discomfort\": self.Avg_Discomfort,\n",
    "            \"Avg_Human_Distance\": self.Avg_Human_Distance, \"Avg_Closest_Human_Distance\": self.Avg_Closest_Human_Distance,\n",
    "            \"Avg_Group_Inhibition_Rate\": self.Avg_Group_Inhibition_Rate\n",
    "        })\n",
    "        \n",
    "        #print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "        #print(f\"RUN DETAILS!!! \\n\")\n",
    "        #print(f\"Ep_Total_Reward : {self.total_reward} \\n\"), \n",
    "        #print(f\"Ep_Collision_Count : {self.collision_count}\")\n",
    "        #print(f\"Ep_Wall_Collision_Count : {self.wall_collision_count}\")\n",
    "        #print(f\"Ep_Obstacle_Collision_Count : {self.obstacle_collision_count}\")\n",
    "        #print(f\"Ep_Human_Collision_Count : {self.human_collision_count}\")\n",
    "        #print(f\"Ep_Min_Time_To_Collision : {self.min_time_to_collision}\")\n",
    "        #print(f\"Ep_Stalled_Time : {self.stalled_time}\")\n",
    "        #print(f\"Ep_Avg_Human_Distance : {self.Avg_Human_Distance}\")\n",
    "        \n",
    "        #print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "        #print(\"Creating the new Episode\")\n",
    "        \n",
    "        if 'seed' in kwargs:\n",
    "            np.random.seed(kwargs['seed'])\n",
    "        \n",
    "        pathfinder = observ.Pathfinding(self.gridworld)\n",
    "        self.obstacle_paths = pathfinder.generate_paths(self.number_of_obstacles, self.episode_length)\n",
    "        self.occupied_time_steps = pathfinder.occupied_time_steps\n",
    "        self.obstacle_occupied_points = pathfinder.obstacle_occupied_points\n",
    "        self.robot_position = None\n",
    "        self.goal_position = None\n",
    "        self.current_step = 0\n",
    "        self.global_obstacle_position = []\n",
    "        self.robot_path = []\n",
    "        self._initialize_robot_and_goal()\n",
    "        \n",
    "        self.agent_pos = self.robot_position\n",
    "        self.goal_pos = self.goal_position\n",
    "        self.num_static_obstacles = 0\n",
    "        self.num_dynamic_obstacles = len(self.obstacle_paths)\n",
    "        \n",
    "        self.dynamic_obstacles = self._init_dynamic_obstacles(self.obstacle_paths)\n",
    "        self.safe = True\n",
    "        self.done = False\n",
    "        \n",
    "        self.Avg_Success_Rate = 0\n",
    "        self.ep_collision_rate, self.collision_count = 0, 0\n",
    "        self.min_time_to_collision = 0\n",
    "        self.ep_wall_collision_rate, self.wall_collision_count = 0, 0\n",
    "        self.ep_obstacle_collision_rate, self.obstacle_collision_count = 0, 0\n",
    "        self.ep_human_collision_rate, self.human_collision_count = 0, 0\n",
    "        self.Avg_Timeout = 0\n",
    "        self.Avg_Path_Length = 0\n",
    "        self.stalled_time = 0\n",
    "        self.ep_group_inhibition_rate, self.group_inhibition = 0, 0\n",
    "        self.ep_discomfort = 0\n",
    "        self.ep_human_distance, self.human_distance = 0, 0\n",
    "        self.closest_human_distance = 0\n",
    "        self.Min_Closest_Human_Distance = 0\n",
    "        self.close_call, self.discomfort, self.current_step = 0, 0, 1\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        #print(\"*******************************************************************\\n\")\n",
    "        #print(\"Initialized the environment with the following\")\n",
    "        #print(\"Agent's Initial Position :\", self.agent_pos)\n",
    "        #print(\"Goal Position :\", self.goal_pos)\n",
    "        #print(\"Number of Dynamic Obstacles :\", self.num_dynamic_obstacles)\n",
    "        #print(\"Dynamic Obstacle theta & dist :\", self.dynamic_obstacles, \"\\n\")\n",
    "        #print(\"*******************************************************************\")\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "\n",
    "class CustomCallback(BaseCallback):\n",
    "    def __init__(self, env, render_freq=1, save_freq=100, save_path=None, verbose=0):\n",
    "        super(CustomCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.render_freq = render_freq\n",
    "        self.save_freq = save_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.render_freq == 0:\n",
    "            self.env.render()\n",
    "        \n",
    "        if self.n_calls % self.save_freq == 0:\n",
    "            save_file = os.path.join(self.save_path, f\"model_step_{self.n_calls}.zip\")\n",
    "            self.model.save(save_file)\n",
    "            if self.verbose > 0:\n",
    "                print(f\"Model saved at step {self.n_calls} to {save_file}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "print(\"Ready\")\n",
    "env = DGSN_Env(gridworld, obstacle_paths, occupied_time_steps, obstacle_occupied_points, number_of_obstacles, episode_length)\n",
    "obs = env.reset()\n",
    "\n",
    "log_path = os.path.join('Train','Logs')\n",
    "save_path = os.path.join('Train', 'Saved_Models', 'PPO_Grid_1')\n",
    "\n",
    "combined_callback = CustomCallback(env=env, render_freq=1, save_freq=100, save_path=save_path, verbose=1)\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1, tensorboard_log=log_path, device='cuda', n_epochs=1000)\n",
    "\n",
    "model.learn(total_timesteps=10000000, callback=combined_callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f246fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ilgc",
   "language": "python",
   "name": "ilgc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
